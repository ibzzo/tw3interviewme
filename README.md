# Test Technique - IngÃ©nieur IA GÃ©nÃ©rative

Ce projet contient les implÃ©mentations pour le test technique d'ingÃ©nieur IA gÃ©nÃ©rative, divisÃ© en trois parties principales.

## ğŸ“ Structure du Projet

```
tw3interviewme/
â”œâ”€â”€ chatbot-ia-generative/    # Partie 1 & 2 : Chatbot avec recherche web
â”‚   â”œâ”€â”€ venv/                # ğŸ Python 3.13 pour Django
â”‚   â”œâ”€â”€ venv_vllm/          # ğŸ Python 3.12 pour vLLM
â”‚   â””â”€â”€ frontend/           # âš›ï¸ React TypeScript
â”œâ”€â”€ vlm_project/             # Partie 3 : DÃ©mo VLM avec Qwen2.5-VL
â”‚   â””â”€â”€ venv_vlm_demo/      # ğŸ Python 3.12 pour VLM demo
â””â”€â”€ README.md               # Ce fichier
```

## ğŸš€ Parties du Test

### Partie 1 & 2 : Chatbot IA avec Recherche Web
Un chatbot intelligent qui combine recherche web en temps rÃ©el et gÃ©nÃ©ration de rÃ©ponses contextuelles.

**FonctionnalitÃ©s principales :**
- Recherche web intelligente multi-sources
- Double mode LLM : vLLM local (Phi-3) ou OpenRouter cloud (Qwen-2.5-32B)
- Panneau de sources interactif
- SÃ©lecteur de modÃ¨le en temps rÃ©el
- Interface React moderne et Ã©purÃ©e

**âš ï¸ Important : Les rÃ©ponses aux questions thÃ©oriques (3.1, 3.2, 4) sont incluses dans le README du chatbot.**

[â†’ Voir les dÃ©tails d'installation et d'utilisation](./chatbot-ia-generative/README.md)

### Partie 3 : Vision Language Model (VLM)
DÃ©monstration d'un modÃ¨le VLM local utilisant Qwen2.5-VL-3B.

**FonctionnalitÃ©s :**
- Analyse d'images locales avec vLLM
- RÃ©ponses aux questions sur le contenu visuel
- OptimisÃ© pour Mac Apple Silicon

[â†’ Voir les dÃ©tails et l'analyse thÃ©orique](./vlm_project/README.md)

## ğŸ“š RÃ©ponses aux Questions ThÃ©oriques

Les rÃ©ponses complÃ¨tes aux questions du test sont disponibles :
- **Question 3.1** (Architecture Azure) : Dans [chatbot-ia-generative/README.md](./chatbot-ia-generative/README.md#31-architecture-et-dÃ©ploiement-azure)
- **Question 3.2** (Optimisation performances) : Dans [chatbot-ia-generative/README.md](./chatbot-ia-generative/README.md#question-32--amÃ©lioration-de-la-vitesse-de-traitement)
- **Question 4** (Architecture VLM) : Dans [chatbot-ia-generative/README.md](./chatbot-ia-generative/README.md#question-4--architecture-vlm-vision-language-model)

## ğŸ›  Installation Rapide

### PrÃ©requis
- Python 3.13 (Django) + Python 3.12 (vLLM)
- Node.js 16+
- 8GB+ de RAM

### Installation ComplÃ¨te (3 environnements)

#### 1. Cloner le projet
```bash
git clone https://github.com/ibzzo/tw3interviewme.git
cd tw3interviewme
```

#### 2. Backend Django (Python 3.13)
```bash
cd chatbot-ia-generative

# CrÃ©er l'environnement virtuel Python 3.13
python3.13 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# Configurer la base de donnÃ©es
python manage.py migrate
```

#### 3. vLLM Local (Python 3.12) - Optionnel
```bash
# IMPORTANT: Rester dans le dossier chatbot-ia-generative

# CrÃ©er un 2Ã¨me environnement virtuel avec Python 3.12
python3.12 -m venv venv_vllm
source venv_vllm/bin/activate  # Linux/Mac
# ou: venv_vllm\Scripts\activate  # Windows

# Installer vLLM (cela installe automatiquement torch, transformers, etc.)
pip install vllm

# DÃ©sactiver pour revenir Ã  l'env principal
deactivate
```

#### 4. Frontend React
```bash
# Retourner dans l'environnement principal
source venv/bin/activate

# Installer les dÃ©pendances frontend
cd frontend
npm install
cd ..
```

#### 5. VLM Demo (3Ã¨me environnement) - Optionnel
```bash
cd ../vlm_project

# CrÃ©er un 3Ã¨me environnement virtuel
python3.12 -m venv venv_vlm_demo
source venv_vlm_demo/bin/activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ“ Configuration

CrÃ©er un fichier `.env` dans `chatbot-ia-generative/` :
```env
OPENROUTER_API_KEY=your_key_here
SERPAPI_API_KEY=your_key_here
```

## ğŸš€ Lancement

### Mode 1 : Cloud OpenRouter uniquement (2 terminaux)
```bash
cd chatbot-ia-generative

# Terminal 1 - Backend Django
source venv/bin/activate        # âš ï¸ Utiliser venv (pas venv_vllm)
python manage.py runserver

# Terminal 2 - Frontend React
cd frontend
npm start
```

### Mode 2 : Avec vLLM local (3 terminaux)
```bash
cd chatbot-ia-generative

# Terminal 1 - vLLM (Python 3.12)
source venv_vllm/bin/activate   # âš ï¸ Utiliser venv_vllm (pas venv)
export VLLM_CPU_KVCACHE_SPACE=8
vllm serve "microsoft/Phi-3-mini-4k-instruct" --host 0.0.0.0 --port 8080 --device cpu

# Terminal 2 - Backend Django (Python 3.13)
source venv/bin/activate        # âš ï¸ Utiliser venv (pas venv_vllm)
python manage.py runserver

# Terminal 3 - Frontend React
cd frontend
npm start
```

### AccÃ¨s
- ğŸŒ **Frontend** : http://localhost:3000
- ğŸ”§ **Backend API** : http://localhost:8000
- ğŸ¤– **vLLM** : http://localhost:8080 (si activÃ©)

### VLM Demo
```bash
cd vlm_project
./start_vllm.sh        # Terminal 1
python vlm_demo.py     # Terminal 2
```

## ğŸ“š Documentation
- [Chatbot - Documentation complÃ¨te](./chatbot-ia-generative/README.md)
- [VLM - Analyse thÃ©orique et pratique](./vlm_project/README.md)

## ğŸ— Architecture

### Chatbot
- **Backend** : Django REST Framework + OpenRouter API
- **Frontend** : React + TypeScript + Styled Components
- **Recherche** : Multi-sources (SerpAPI, DuckDuckGo, Tavily)

### VLM
- **ModÃ¨le** : Qwen2.5-VL-3B-Instruct
- **Serveur** : vLLM optimisÃ© pour CPU
- **Interface** : Script Python interactif

## ğŸ‘¤ Auteur
Ibrahim Adiao

## ğŸ“„ License
Ce projet est dÃ©veloppÃ© dans le cadre d'un test technique.
# Partie 3 : Vision Language Model (VLM) - Qwen2.5-VL

## üìã Description

Ce projet impl√©mente la partie 3 du test technique : un script Python utilisant **Qwen/Qwen2.5-VL-3B-Instruct** en local pour traiter des images.

## üöÄ Installation

### Pr√©requis
- Mac M4 avec 24GB RAM
- Homebrew install√©
- Python 3.12 (vLLM ne supporte pas Python 3.13)

### Installation manuelle

```bash
# 1. Installer Python 3.12
brew install python@3.12

# 2. Cr√©er l'environnement virtuel
python3.12 -m venv venv_vlm
source venv_vlm/bin/activate

# 3. Installer les d√©pendances
pip install vllm requests pillow
```

## üíª Utilisation

### 1. D√©marrer le serveur vLLM

```bash
# Utiliser le script optimis√©
./start_vllm.sh

# OU manuellement
source venv_vlm/bin/activate
export VLLM_CPU_KVCACHE_SPACE=8
vllm serve "Qwen/Qwen2.5-VL-3B-Instruct" --max-model-len 8192
```

### 2. Dans un nouveau terminal, ex√©cuter le script

```bash
source venv_vlm/bin/activate

# Avec une image locale
python vlm_demo.py photo.jpg

# Avec un prompt personnalis√©
python vlm_demo.py photo.jpg "Extract all text from this image"

# Avec une URL
python vlm_demo.py https://example.com/image.jpg
```

## üìÅ Structure

```
vlm_project/
‚îú‚îÄ‚îÄ README.md              # Ce fichier
‚îú‚îÄ‚îÄ vlm_demo.py           # Script principal VLM
‚îú‚îÄ‚îÄ start_vllm.sh         # Script de d√©marrage optimis√©
‚îî‚îÄ‚îÄ venv_vlm/             # Environnement virtuel (ignor√© par git)
```

## ‚ö†Ô∏è Notes importantes

- **Python 3.13 n'est PAS support√©** par vLLM
- Le premier chargement du mod√®le peut prendre plusieurs minutes
- Le mod√®le utilise environ 6-8GB de RAM
- Optimis√© pour Mac M4 avec acc√©l√©ration Metal

## üéØ Fonctionnalit√©s

- ‚úÖ Description d√©taill√©e d'images
- ‚úÖ Extraction de texte (OCR)
- ‚úÖ Support images locales et URLs
- ‚úÖ Prompts personnalisables
- ‚úÖ Utilisation de Qwen2.5-VL-3B-Instruct

## üèóÔ∏è Question th√©orique : Architecture VLM pour syst√®me RAG

### Comment architecturer un syst√®me de vectorisation de documents int√©grant des VLMs ?

Pour cr√©er un pipeline RAG multimodal efficace, l'architecture doit r√©soudre trois d√©fis majeurs : l'extraction intelligente du contenu, la pr√©servation des relations contextuelles, et la recherche unifi√©e cross-modale.

### 1. Architecture propos√©e

Le syst√®me s'organise autour de 5 composants principaux interconnect√©s :

**Extracteur Multimodal** ‚Üí **Pr√©processeur VLM** ‚Üí **Vectorisation Hybride** ‚Üí **Stockage Index√©** ‚Üí **Moteur de Recherche**

### 2. Composants cl√©s et leurs r√¥les

#### 2.1 Extraction intelligente
L'extracteur doit identifier non seulement le texte et les images, mais surtout leurs relations spatiales et s√©mantiques. Pour un graphique accompagn√© d'une l√©gende, le syst√®me doit maintenir ce lien tout au long du pipeline. Les outils comme LayoutLM comprennent la structure visuelle des documents pour pr√©server ces associations.

#### 2.2 Enrichissement par VLM
Les VLMs transforment les images en descriptions textuelles riches. Un graphique financier devient "courbe ascendante montrant une croissance de 23% sur Q3 2024". Le VLM analyse aussi le contexte : une image pr√®s d'un paragraphe sur les ventes sera interpr√©t√©e diff√©remment qu'une pr√®s d'un texte technique.

#### 2.3 Vectorisation multi-modale
Trois types de vecteurs sont g√©n√©r√©s :
- **Vecteurs textuels** : embeddings classiques pour le contenu textuel
- **Vecteurs visuels** : repr√©sentations CLIP des images
- **Vecteurs unifi√©s** : fusion cross-modale pour recherche conceptuelle

Cette approche permet de chercher par texte ("graphique des ventes"), par similarit√© visuelle, ou par concept abstrait ("tendance positive").

#### 2.4 Indexation strat√©gique
Le stockage utilise des index s√©par√©s par modalit√© avec m√©tadonn√©es enrichies :
- Position dans le document
- Relations avec autres √©l√©ments
- Descriptions VLM
- Scores de confiance

Cette structure permet des requ√™tes complexes comme "trouver tous les diagrammes techniques avec leurs explications associ√©es".

#### 2.5 Recherche hybride intelligente
Le moteur combine plusieurs strat√©gies :
- Expansion de requ√™te pour couvrir les synonymes visuels
- Fusion pond√©r√©e des r√©sultats multi-modaux
- Reranking cross-encoder pour pertinence finale
- Diversification MMR pour √©viter la redondance

### 3. Optimisations critiques

**Chunking contextuel** : Les segments preservent les liens texte-image dans une fen√™tre de proximit√© (ex: 200 mots avant/apr√®s).

**Cache perceptuel** : √âvite de retraiter des images similaires en utilisant des hash perceptuels avec seuil de similarit√©.

**D√©tection d'hallucinations** : Valide que les descriptions VLM correspondent r√©ellement au contenu visuel pour maintenir la qualit√©.

**Traitement asynchrone** : Les op√©rations VLM co√ªteuses sont distribu√©es pour maintenir la latence acceptable.

### 4. Avantages de cette approche

1. **Compr√©hension holistique** : Le syst√®me comprend les documents comme un humain, en liant naturellement texte et visuels.

2. **Flexibilit√© de recherche** : Les utilisateurs peuvent chercher par description textuelle d'images qu'ils n'ont jamais vues.

3. **Pr√©cision contextuelle** : Les r√©sultats incluent automatiquement les √©l√©ments visuels pertinents avec leur contexte.

4. **Scalabilit√©** : L'architecture modulaire permet d'optimiser chaque composant ind√©pendamment.

### 5. Consid√©rations pratiques

Pour la production, privil√©gier :
- VLMs locaux pour le volume (Qwen2.5-VL sur GPU)
- APIs cloud (GPT-4V) uniquement pour cas complexes
- Monitoring de la latence par modalit√©
- Strat√©gie de cache aggressive (24h minimum)

Cette architecture transforme le RAG traditionnel en syst√®me v√©ritablement multimodal, o√π texte et images sont trait√©s comme citoyens de premi√®re classe, offrant une exp√©rience de recherche naturelle et compl√®te.
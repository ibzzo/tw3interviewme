# Vision Language Model (VLM) - Qwen2.5-VL

[‚Üê Retour au projet principal](../README.md)

Impl√©mentation de la partie 3 du test technique : d√©monstration d'un VLM local avec Qwen2.5-VL-3B.

## üéØ Objectif

Cr√©er un script Python utilisant un Vision Language Model local pour :
- Analyser des images locales
- Extraire du texte (OCR)
- R√©pondre √† des questions sur le contenu visuel

## üöÄ Installation

### Pr√©requis
- Mac Apple Silicon (test√© sur M4)
- Python 3.12 (‚ö†Ô∏è vLLM ne supporte pas Python 3.13)
- 8GB+ de RAM disponible

### Installation

L'environnement vLLM est partag√© avec le chatbot. Voir l'installation compl√®te dans le [README principal](../README.md).

## üíª Utilisation

### √âtape 1 : D√©marrer le serveur vLLM

```bash
# Depuis la racine du projet
source venv_vllm/bin/activate
cd vlm_project
./start_vllm.sh
```

Le serveur d√©marre sur http://localhost:8000. Attendez le message "Uvicorn running" avant de continuer.

### √âtape 2 : Pr√©parer une image test

```bash
# Copier une image de votre choix dans le dossier vlm_project
# Par exemple :
cp ~/Desktop/mon_image.jpg ./test_image.jpg

# Ou t√©l√©charger une image d'exemple :
curl -o test_image.jpg https://images.unsplash.com/photo-1574158622682-e40e69881006
```

### √âtape 3 : Ex√©cuter le script VLM

Dans un nouveau terminal :

```bash
# Depuis la racine du projet
source venv_vllm/bin/activate
cd vlm_project

# Analyse basique d'image (remplacer test_image.jpg par votre nom de fichier)
python vlm_demo.py test_image.jpg

# Avec prompt personnalis√©
python vlm_demo.py test_image.jpg "Extract all text from this document"

# Avec URL d'image
python vlm_demo.py https://example.com/image.jpg
```

## üìÅ Fichiers du projet

- **`vlm_demo.py`** : Script principal avec animation de chargement
- **`start_vllm.sh`** : Script optimis√© pour d√©marrer vLLM sur CPU
- **`README.md`** : Documentation et analyse th√©orique

## ‚ö° Optimisations appliqu√©es

1. **Configuration CPU** : `VLLM_CPU_KVCACHE_SPACE=8` pour Mac
2. **Mod√®le r√©duit** : `max-model-len=8192` pour √©conomiser la RAM
3. **Timeout ajust√©** : 300s pour traitement CPU
4. **Animation visuelle** : Feedback durant le traitement

## üèóÔ∏è Question th√©orique : Architecture VLM-RAG

### Comment int√©grer des VLMs dans un syst√®me RAG ?

Pour cr√©er un pipeline RAG multimodal efficace, l'architecture doit r√©soudre trois d√©fis : extraction intelligente, pr√©servation des relations contextuelles, et recherche unifi√©e.

### Architecture propos√©e

```
Documents ‚Üí Extracteur ‚Üí VLM Enrichissement ‚Üí Vectorisation ‚Üí Index ‚Üí Recherche
```

### Composants cl√©s

**1. Extraction intelligente**
- Identifie texte, images et leurs relations spatiales
- Pr√©serve les liens (ex: graphique + l√©gende)
- Utilise LayoutLM pour comprendre la structure

**2. Enrichissement VLM**
- Transforme images en descriptions textuelles
- Analyse le contexte pour interpr√©ter correctement
- Exemple : graphique ‚Üí "croissance 23% Q3 2024"

**3. Vectorisation hybride**
- Vecteurs textuels : embeddings standards
- Vecteurs visuels : repr√©sentations CLIP
- Vecteurs unifi√©s : fusion cross-modale

**4. Indexation strat√©gique**
```python
{
  "content": "texte ou description",
  "type": "text|image|unified",
  "embedding": [0.1, 0.2, ...],
  "metadata": {
    "position": 42,
    "relations": ["fig_1", "para_3"],
    "vlm_confidence": 0.95
  }
}
```

**5. Recherche intelligente**
- Expansion de requ√™te automatique
- Fusion pond√©r√©e multi-modale
- Reranking pour pertinence finale

### Optimisations pratiques

1. **Chunking contextuel** : Fen√™tre de 200 mots autour des images
2. **Cache perceptuel** : Hash des images pour √©viter retraitement
3. **Validation VLM** : D√©tection d'hallucinations
4. **Pipeline asynchrone** : Traitement parall√®le

### Avantages

‚úÖ Compr√©hension holistique des documents  
‚úÖ Recherche flexible (texte ‚Üí image)  
‚úÖ Contexte pr√©serv√© automatiquement  
‚úÖ Architecture scalable et modulaire  

### En production

- VLMs locaux (Qwen2.5-VL) pour volume
- APIs cloud (GPT-4V) pour cas complexes
- Cache aggressive (24h+)
- Monitoring par modalit√©

Cette approche transforme le RAG en syst√®me v√©ritablement multimodal, offrant une recherche naturelle o√π texte et images sont trait√©s √©quitablement.

## üìù Notes techniques

- Premier chargement : ~2-3 minutes
- Consommation RAM : ~6-8GB
- Optimis√© pour Apple Silicon (Metal)
- Support images locales et URLs

---

Pour le chatbot avec recherche web, voir [chatbot-ia-generative](../chatbot-ia-generative/README.md)
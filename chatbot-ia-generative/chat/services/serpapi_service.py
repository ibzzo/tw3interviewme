import os
from serpapi import GoogleSearch
from typing import List, Dict, Optional
import logging
from datetime import datetime, timedelta
from django.conf import settings
from chat.models import SearchCache
import re
import json

logger = logging.getLogger(__name__)


class SerpAPIService:
    """Service intelligent pour recherche web via SerpAPI avec optimisations avanc√©es."""
    
    def __init__(self):
        self.api_key = os.environ.get('SERPAPI_KEY', '8ba4cd7cae7dab8bab44ee1ea895b405552d5b956d2b725122084e5a081eaf9f')
        self.max_results = settings.MAX_SEARCH_RESULTS
        
        # Strat√©gies de recherche par type de requ√™te
        self.search_strategies = {
            'news': self._search_news_strategy,
            'technical': self._search_technical_strategy,
            'general': self._search_general_strategy,
            'academic': self._search_academic_strategy
        }
    
    def analyze_query_intent(self, query: str) -> Dict[str, any]:
        """Analyse l'intention de la requ√™te pour optimiser la recherche."""
        query_lower = query.lower()
        
        # D√©tection du type de recherche
        if any(word in query_lower for word in ['derniers', 'r√©cents', 'news', 'actualit√©s', 'cette semaine', 'aujourd\'hui', 'latest', 'recent', 'annonc√©s', 'd√©veloppements']):
            search_type = 'news'
            time_filter = 'd'  # Derni√®res 24h pour plus de fra√Æcheur
        elif any(word in query_lower for word in ['comment', 'tutoriel', 'how to', 'guide', 'implementation']):
            search_type = 'technical'
            time_filter = 'm'  # Dernier mois
        elif any(word in query_lower for word in ['recherche', '√©tude', 'paper', 'research', 'academic']):
            search_type = 'academic'
            time_filter = 'y'  # Derni√®re ann√©e
        else:
            search_type = 'general'
            time_filter = None
        
        # Extraction des mots-cl√©s importants
        keywords = self._extract_keywords(query)
        
        # D√©tection de la langue
        language = 'fr' if any(word in query_lower for word in ['quels', 'comment', 'pourquoi', 'd√©veloppements']) else 'en'
        
        return {
            'type': search_type,
            'time_filter': time_filter,
            'keywords': keywords,
            'language': language,
            'original_query': query
        }
    
    def _extract_keywords(self, query: str) -> List[str]:
        """Extrait les mots-cl√©s importants de la requ√™te."""
        # Mots √† ignorer
        stop_words = {
            'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'et', 'ou', 'mais',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'quels', 'sont', 'what', 'are', 'is', 'how', 'comment', 'pourquoi'
        }
        
        # Extraction des mots importants
        words = re.findall(r'\b\w+\b', query.lower())
        keywords = [w for w in words if w not in stop_words and len(w) > 2]
        
        # Prioriser les mots techniques/sp√©cifiques
        tech_terms = ['ia', 'ai', 'g√©n√©rative', 'generative', 'llm', 'gpt', 'claude', 
                      'machine learning', 'deep learning', 'neural', 'transformer']
        
        important_keywords = [k for k in keywords if any(term in k for term in tech_terms)]
        
        return important_keywords[:5] if important_keywords else keywords[:5]
    
    def search(self, query: str, search_type: str = None, use_cache: bool = True) -> List[Dict]:
        """
        Recherche intelligente avec SerpAPI.
        """
        logger.info(f"\nüîç SERPAPI - RECHERCHE INTELLIGENTE")
        
        # V√©rifier le cache
        if use_cache:
            cached = self._get_cached_results(query)
            if cached:
                logger.info(f"üì¶ Utilisation du cache pour: {query[:50]}...")
                return cached
        
        # Analyser l'intention de la requ√™te
        intent = self.analyze_query_intent(query)
        # Override avec search_type si fourni
        if search_type:
            intent['type'] = search_type
        logger.info(f"üß† Analyse: Type={intent['type']}, Langue={intent['language']}, Mots-cl√©s={intent['keywords']}")
        
        # Utiliser la strat√©gie appropri√©e
        strategy = self.search_strategies.get(intent['type'], self._search_general_strategy)
        results = strategy(intent)
        
        # Enrichir et scorer les r√©sultats
        if results:
            results = self._enrich_and_score_results(results, intent)
            
            # Afficher les r√©sultats dans la console pour debug
            logger.info("\n" + "üåé"*40)
            logger.info("üéØ R√âSULTATS SERPAPI")
            logger.info("üåé"*40)
            for i, r in enumerate(results):
                logger.info(f"\nüìå R√©sultat {i+1}:")
                logger.info(f"   üìù Titre: {r['title']}")
                logger.info(f"   üåê URL: {r['url']}")
                logger.info(f"   üè¢ Source: {r['source']}")
                logger.info(f"   üìÖ Date: {r.get('date', 'N/A')}")
                logger.info(f"   üìä Score: {r.get('relevance_score', 0):.1%}")
                logger.info(f"   üè∑Ô∏è Tags: {', '.join(r.get('tags', []))}")
                logger.info(f"   üí¨ Contenu: {r['content'][:200]}...")
            logger.info("üåé"*40 + "\n")
            
            # Mettre en cache
            if use_cache:
                self._cache_results(query, results)
        
        return results
    
    def _search_news_strategy(self, intent: Dict) -> List[Dict]:
        """Strat√©gie optimis√©e pour les actualit√©s."""
        logger.info(f"üì∞ Strat√©gie NEWS activ√©e")
        
        # Requ√™tes multiples pour couvrir diff√©rents angles
        queries = [
            # Requ√™te principale cibl√©e sur les annonces r√©centes
            f"\"AI announcements\" OR \"IA annonces\" {intent['original_query']} -filetype:pdf",
            # Requ√™te sp√©cifique aux grandes entreprises
            f"(OpenAI OR Anthropic OR Google OR Microsoft OR Meta) AI announcement 2025",
            # Requ√™te sur les mod√®les sp√©cifiques
            f"GPT-5 OR Claude-3 OR Gemini OR LLaMA new model release 2025"
        ]
        
        all_results = []
        
        for query in queries:
            params = {
                "q": query,
                "api_key": self.api_key,
                "tbm": "nws",  # Google News
                "tbs": "qdr:w,sbd:1",  # Derni√®re semaine, tri√©s par date
                "num": 20,  # Plus de r√©sultats pour filtrer
                "hl": intent['language'],
                "gl": "fr" if intent['language'] == 'fr' else "us"
            }
        
            try:
                search = GoogleSearch(params)
                results = search.get_dict()
                
                if "news_results" in results:
                    for item in results["news_results"]:
                        # √âviter les doublons
                        if not any(r['url'] == item.get('link', '') for r in all_results):
                            date_parsed = self._parse_serpapi_date(item.get('date', ''))
                            all_results.append({
                                'title': item.get('title', ''),
                                'url': item.get('link', ''),
                                'content': item.get('snippet', ''),
                                'source': item.get('source', {}).get('name', '') if isinstance(item.get('source'), dict) else item.get('source', ''),
                                'date': item.get('date', ''),
                                'date_parsed': date_parsed.isoformat() if date_parsed else None,
                                'relevance_score': self._calculate_news_relevance(item, intent)
                            })
            except Exception as e:
                logger.error(f"‚ùå Erreur pour requ√™te '{query[:50]}...': {str(e)}")
        
        # Si pas assez de r√©sultats news, chercher aussi dans les r√©sultats web r√©cents
        if len(all_results) < 5:
            logger.info("üîÑ Recherche compl√©mentaire dans les r√©sultats web")
            web_params = {
                "q": f'{intent["original_query"]} "announced today" OR "announced yesterday" OR "launching" AI',
                "api_key": self.api_key,
                "num": 10,
                "tbs": "qdr:d",  # Derni√®res 24h
                "hl": intent['language']
            }
            
            try:
                search = GoogleSearch(web_params)
                results = search.get_dict()
                
                if "organic_results" in results:
                    for item in results["organic_results"]:
                        # V√©rifier si c'est vraiment une actualit√© r√©cente
                        snippet = item.get('snippet', '').lower()
                        if any(word in snippet for word in ['today', 'yesterday', 'announced', 'launches', 'releases', 'introduces']):
                            date_parsed = datetime.now() - timedelta(hours=12)  # Estimation r√©cente
                            all_results.append({
                                'title': item.get('title', ''),
                                'url': item.get('link', ''),
                                'content': item.get('snippet', ''),
                                'source': self._extract_domain(item.get('link', '')),
                                'date': 'Recent',
                                'date_parsed': date_parsed.isoformat(),
                                'relevance_score': self._calculate_news_relevance(item, intent) * 0.9  # L√©g√®rement moins pertinent
                            })
            except Exception as e:
                logger.error(f"‚ùå Erreur recherche web compl√©mentaire: {str(e)}")
        
        # Trier par pertinence et date
        all_results.sort(key=lambda x: (x['relevance_score'], self._parse_date_priority(x.get('date', ''))), reverse=True)
        
        # D√©dupliquer et prendre les meilleurs
        unique_results = []
        seen_titles = set()
        for result in all_results:
            title_key = result['title'][:50].lower()
            if title_key not in seen_titles:
                unique_results.append(result)
                seen_titles.add(title_key)
        
        logger.info(f"‚úÖ {len(unique_results)} actualit√©s uniques trouv√©es")
        return unique_results[:self.max_results]
    
    def _search_technical_strategy(self, intent: Dict) -> List[Dict]:
        """Strat√©gie pour recherches techniques/tutoriels."""
        logger.info(f"üîß Strat√©gie TECHNIQUE activ√©e")
        
        # Ajouter des sites techniques de r√©f√©rence
        tech_sites = "site:github.com OR site:stackoverflow.com OR site:medium.com OR site:dev.to"
        enhanced_query = f"{intent['original_query']} tutorial implementation code {tech_sites}"
        
        params = {
            "q": enhanced_query,
            "api_key": self.api_key,
            "num": 10,
            "hl": intent['language']
        }
        
        try:
            search = GoogleSearch(params)
            results = search.get_dict()
            
            formatted_results = []
            if "organic_results" in results:
                for item in results["organic_results"]:
                    formatted_results.append({
                        'title': item.get('title', ''),
                        'url': item.get('link', ''),
                        'content': item.get('snippet', ''),
                        'source': self._extract_domain(item.get('link', '')),
                        'relevance_score': self._calculate_relevance(item, intent)
                    })
            
            # Prioriser GitHub et StackOverflow
            formatted_results.sort(key=lambda x: (
                x['relevance_score'],
                'github.com' in x['url'],
                'stackoverflow.com' in x['url']
            ), reverse=True)
            
            logger.info(f"‚úÖ {len(formatted_results)} r√©sultats techniques trouv√©s")
            return formatted_results[:self.max_results]
            
        except Exception as e:
            logger.error(f"‚ùå Erreur SerpAPI Technical: {str(e)}")
            return []
    
    def _search_general_strategy(self, intent: Dict) -> List[Dict]:
        """Strat√©gie de recherche g√©n√©rale avec AI Overview."""
        logger.info(f"üåê Strat√©gie G√âN√âRALE activ√©e")
        
        params = {
            "q": intent['original_query'],
            "api_key": self.api_key,
            "num": 10,
            "hl": intent['language'],
            "gl": "fr" if intent['language'] == 'fr' else "us"
        }
        
        try:
            search = GoogleSearch(params)
            results = search.get_dict()
            
            formatted_results = []
            
            # Extraire l'AI Overview si disponible
            if "ai_overview" in results:
                logger.info("ü§ñ AI Overview trouv√©!")
                ai_content = results["ai_overview"].get("text", "")
                if ai_content:
                    formatted_results.append({
                        'title': "R√©sum√© IA de Google",
                        'url': "https://google.com",
                        'content': ai_content,
                        'source': "Google AI",
                        'relevance_score': 1.0  # Tr√®s pertinent
                    })
            
            # R√©sultats organiques
            if "organic_results" in results:
                for item in results["organic_results"]:
                    formatted_results.append({
                        'title': item.get('title', ''),
                        'url': item.get('link', ''),
                        'content': item.get('snippet', ''),
                        'source': self._extract_domain(item.get('link', '')),
                        'relevance_score': self._calculate_relevance(item, intent)
                    })
            
            # Trier par pertinence
            formatted_results.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            logger.info(f"‚úÖ {len(formatted_results)} r√©sultats g√©n√©raux trouv√©s")
            return formatted_results[:self.max_results]
            
        except Exception as e:
            logger.error(f"‚ùå Erreur SerpAPI General: {str(e)}")
            return []
    
    def _search_academic_strategy(self, intent: Dict) -> List[Dict]:
        """Strat√©gie pour recherches acad√©miques."""
        logger.info(f"üéì Strat√©gie ACAD√âMIQUE activ√©e")
        
        # Utiliser Google Scholar
        params = {
            "q": intent['original_query'],
            "api_key": self.api_key,
            "engine": "google_scholar",
            "num": 10,
            "hl": intent['language']
        }
        
        try:
            search = GoogleSearch(params)
            results = search.get_dict()
            
            formatted_results = []
            if "organic_results" in results:
                for item in results["organic_results"]:
                    formatted_results.append({
                        'title': item.get('title', ''),
                        'url': item.get('link', ''),
                        'content': item.get('snippet', ''),
                        'source': item.get('publication_info', {}).get('summary', 'Academic'),
                        'relevance_score': self._calculate_relevance(item, intent)
                    })
            
            logger.info(f"‚úÖ {len(formatted_results)} r√©sultats acad√©miques trouv√©s")
            return formatted_results[:self.max_results]
            
        except Exception as e:
            logger.error(f"‚ùå Erreur SerpAPI Academic: {str(e)}")
            # Fallback vers recherche g√©n√©rale
            return self._search_general_strategy(intent)
    
    def _calculate_relevance(self, result: Dict, intent: Dict) -> float:
        """Calcule un score de pertinence sophistiqu√©."""
        score = 0.0
        
        title = result.get('title', '').lower()
        content = result.get('snippet', '').lower()
        url = result.get('link', '').lower()
        
        # Score bas√© sur les mots-cl√©s
        for keyword in intent['keywords']:
            if keyword in title:
                score += 0.3
            if keyword in content:
                score += 0.2
            if keyword in url:
                score += 0.1
        
        # Bonus pour la fra√Æcheur (news)
        if intent['type'] == 'news' and 'date' in result:
            try:
                # Favoriser les r√©sultats r√©cents
                date_str = result['date']
                if 'hour' in date_str or 'heure' in date_str:
                    score += 0.3
                elif 'day' in date_str or 'jour' in date_str:
                    score += 0.2
            except:
                pass
        
        # Bonus pour sources fiables
        trusted_sources = ['openai.com', 'anthropic.com', 'google.com', 'microsoft.com', 
                          'github.com', 'arxiv.org', 'nature.com', 'science.org']
        
        if any(source in url for source in trusted_sources):
            score += 0.2
        
        return min(score, 1.0)  # Normaliser entre 0 et 1
    
    def _calculate_news_relevance(self, result: Dict, intent: Dict) -> float:
        """Calcule un score de pertinence sp√©cifique pour les actualit√©s."""
        score = 0.0
        
        title = result.get('title', '').lower()
        content = result.get('snippet', '').lower()
        url = result.get('link', '').lower()
        
        # Mots-cl√©s d'actualit√© importants
        news_keywords = [
            'announces', 'announced', 'launches', 'launched', 'releases', 'released',
            'introduces', 'unveils', 'reveals', 'debuts', 'new', 'latest',
            'annonce', 'lance', 'd√©voile', 'pr√©sente', 'nouveau', 'derni√®re'
        ]
        
        # Score pour mots-cl√©s d'actualit√©
        for keyword in news_keywords:
            if keyword in title:
                score += 0.2
            if keyword in content:
                score += 0.1
        
        # Score pour entreprises IA majeures
        ai_companies = [
            'openai', 'anthropic', 'google', 'microsoft', 'meta', 'nvidia',
            'hugging face', 'stability ai', 'cohere', 'inflection'
        ]
        
        for company in ai_companies:
            if company in title or company in content:
                score += 0.3
                break
        
        # Score pour mod√®les IA sp√©cifiques
        ai_models = [
            'gpt', 'claude', 'gemini', 'llama', 'palm', 'bard', 'copilot',
            'stable diffusion', 'midjourney', 'dall-e'
        ]
        
        for model in ai_models:
            if model in title or model in content:
                score += 0.2
                break
        
        # Score pour la fra√Æcheur
        date_str = result.get('date', '').lower()
        if any(x in date_str for x in ['hour', 'heure', 'minute']):
            score += 0.4  # Tr√®s r√©cent
        elif any(x in date_str for x in ['today', "aujourd'hui", '1 day', '1 jour']):
            score += 0.3
        elif any(x in date_str for x in ['yesterday', 'hier', '2 days', '2 jours']):
            score += 0.2
        elif any(x in date_str for x in ['3 days', '3 jours', '4 days', '4 jours']):
            score += 0.1
        
        # P√©nalit√© pour contenus non pertinents
        irrelevant_keywords = ['course', 'tutorial', 'how to', 'guide', 'cours', 'tutoriel']
        if any(keyword in title.lower() for keyword in irrelevant_keywords):
            score *= 0.5
        
        return min(score, 1.0)
    
    def _parse_date_priority(self, date_str: str) -> int:
        """Parse la date pour le tri (plus r√©cent = priorit√© plus √©lev√©e)."""
        date_lower = date_str.lower()
        
        if any(x in date_lower for x in ['minute', 'hour', 'heure']):
            return 10
        elif any(x in date_lower for x in ['today', "aujourd'hui"]):
            return 9
        elif any(x in date_lower for x in ['yesterday', 'hier']):
            return 8
        elif '1 day' in date_lower or '1 jour' in date_lower:
            return 7
        elif '2 day' in date_lower or '2 jour' in date_lower:
            return 6
        elif '3 day' in date_lower or '3 jour' in date_lower:
            return 5
        else:
            return 0
    
    def _enrich_and_score_results(self, results: List[Dict], intent: Dict) -> List[Dict]:
        """Enrichit les r√©sultats avec des m√©tadonn√©es suppl√©mentaires."""
        enriched = []
        
        for result in results:
            # Ajouter un r√©sum√© si le contenu est long
            content = result.get('content', '')
            if len(content) > 200:
                result['summary'] = content[:197] + "..."
            
            # Ajouter des tags bas√©s sur le contenu
            result['tags'] = self._extract_tags(result)
            
            # Score final
            if 'relevance_score' not in result:
                result['relevance_score'] = self._calculate_relevance(result, intent)
            
            enriched.append(result)
        
        # Retourner les meilleurs r√©sultats
        enriched.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        return enriched
    
    def _extract_tags(self, result: Dict) -> List[str]:
        """Extrait des tags du contenu."""
        tags = []
        content = (result.get('title', '') + ' ' + result.get('content', '')).lower()
        
        # Tags IA
        ai_tags = {
            'gpt': 'GPT', 'claude': 'Claude', 'gemini': 'Gemini', 
            'llm': 'LLM', 'transformer': 'Transformer', 'bert': 'BERT'
        }
        
        for keyword, tag in ai_tags.items():
            if keyword in content:
                tags.append(tag)
        
        return tags[:3]  # Limiter √† 3 tags
    
    def _extract_domain(self, url: str) -> str:
        """Extrait le domaine d'une URL."""
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            return domain.replace('www.', '')
        except:
            return 'Unknown'
    
    def _get_cached_results(self, query: str) -> Optional[List[Dict]]:
        """R√©cup√®re les r√©sultats en cache."""
        try:
            cache_entry = SearchCache.objects.filter(query=query).first()
            if cache_entry and not cache_entry.is_expired(6):  # Cache 6h pour SerpAPI
                return cache_entry.results
        except Exception as e:
            logger.error(f"Erreur cache: {e}")
        return None
    
    def _cache_results(self, query: str, results: List[Dict]):
        """Met en cache les r√©sultats."""
        try:
            SearchCache.objects.update_or_create(
                query=query,
                defaults={'results': results}
            )
        except Exception as e:
            logger.error(f"Erreur mise en cache: {e}")
    
    def get_trending_topics(self) -> List[Dict]:
        """R√©cup√®re les sujets tendances en IA."""
        params = {
            "q": "artificial intelligence latest news",
            "api_key": self.api_key,
            "tbm": "nws",
            "num": 5,
            "tbs": "qdr:d"  # Derni√®res 24h
        }
        
        try:
            search = GoogleSearch(params)
            results = search.get_dict()
            
            topics = []
            if "news_results" in results:
                for item in results["news_results"]:
                    topics.append({
                        'title': item.get('title', ''),
                        'trend': 'hot' if 'hour' in item.get('date', '') else 'warm'
                    })
            
            return topics
            
        except Exception as e:
            logger.error(f"Erreur trending topics: {e}")
            return []
    
    def _parse_serpapi_date(self, date_str: str) -> datetime:
        """Parse les dates de SerpAPI en datetime."""
        if not date_str:
            return None
            
        date_lower = date_str.lower()
        now = datetime.now()
        
        try:
            # Patterns relatifs
            if 'minute' in date_lower or 'min ago' in date_lower:
                # Extraire le nombre de minutes
                match = re.search(r'(\d+)\s*min', date_lower)
                if match:
                    minutes = int(match.group(1))
                    return now - timedelta(minutes=minutes)
                return now - timedelta(minutes=30)  # Par d√©faut
                
            elif 'hour' in date_lower or 'heure' in date_lower:
                match = re.search(r'(\d+)\s*h', date_lower)
                if match:
                    hours = int(match.group(1))
                    return now - timedelta(hours=hours)
                return now - timedelta(hours=1)
                
            elif 'today' in date_lower or "aujourd'hui" in date_lower:
                return now.replace(hour=12, minute=0, second=0)
                
            elif 'yesterday' in date_lower or 'hier' in date_lower:
                return now - timedelta(days=1)
                
            elif 'day' in date_lower or 'jour' in date_lower:
                match = re.search(r'(\d+)\s*day', date_lower)
                if match:
                    days = int(match.group(1))
                    return now - timedelta(days=days)
                    
            elif 'week' in date_lower or 'semaine' in date_lower:
                match = re.search(r'(\d+)\s*week', date_lower)
                if match:
                    weeks = int(match.group(1))
                    return now - timedelta(weeks=weeks)
                    
            # Formats de date absolus
            # Format: Jan 15, 2025
            date_pattern = r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{1,2}),?\s+(\d{4})'
            match = re.search(date_pattern, date_str, re.IGNORECASE)
            if match:
                month_abbr = {
                    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,
                    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12
                }
                month = month_abbr[match.group(1).lower()]
                day = int(match.group(2))
                year = int(match.group(3))
                return datetime(year, month, day)
                
        except Exception as e:
            logger.error(f"Erreur parsing date '{date_str}': {e}")
            
        return None
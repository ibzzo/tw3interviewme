"""
Service de recherche intelligent avec g√©n√©ration de requ√™te par LLM
"""
import httpx
import json
import logging
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
from django.conf import settings

from .serpapi_service import SerpAPIService
from .multi_search import MultiSearchService
try:
    from .huggingface_lite import HuggingFaceLiteService
except ImportError:
    HuggingFaceLiteService = None

logger = logging.getLogger(__name__)


class IntelligentSearchService:
    """Service de recherche intelligent qui utilise le LLM pour optimiser les requ√™tes"""
    
    def __init__(self):
        # On n'utilise plus OpenRouter pour le LLM principal
        # Services de recherche
        self.serpapi_service = SerpAPIService()
        self.multi_search = MultiSearchService()
        
        # Service LLM avec Hugging Face Lite
        logger.info("\nüß† CONFIGURATION DU SERVICE LLM")
        logger.info("-" * 40)
        
        self.use_huggingface = False
        if HuggingFaceLiteService:
            logger.info("üîç Tentative d'utilisation de Hugging Face...")
            try:
                self.llm_service = HuggingFaceLiteService()
                if self.llm_service.is_available():
                    self.use_huggingface = True
                    logger.info("‚úÖ SERVICE S√âLECTIONN√â: Hugging Face (LLM Local)")
                    logger.info("üéØ Vous gagnez +5 points bonus pour l'utilisation d'un LLM local!")
                else:
                    logger.info("‚ö†Ô∏è Hugging Face non disponible")
                    logger.info("üîÑ Basculement vers OpenRouter")
            except Exception as e:
                logger.error(f"‚ùå Erreur initialisation Hugging Face: {e}")
                logger.info("üîÑ Basculement vers OpenRouter")
        else:
            logger.info("‚ùå Module HuggingFaceLiteService non trouv√©")
        
        # Configuration OpenRouter comme fallback
        if not self.use_huggingface:
            logger.info("üåê SERVICE S√âLECTIONN√â: OpenRouter (API Externe)")
            logger.info("üí° Pour utiliser un LLM local (+5 points bonus):")
            logger.info("   pip install transformers torch accelerate")
            self.api_key = settings.OPENROUTER_API_KEY
            self.base_url = settings.OPENROUTER_BASE_URL
            self.model = settings.OPENROUTER_MODEL
            self.headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "http://localhost:3000",
                "X-Title": "AI Chatbot Intelligent Search"
            }
    
    def process_user_query(
        self,
        user_query: str,
        time_constraint: Optional[str] = None,
        current_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """
        Traite la requ√™te utilisateur en 2 √©tapes :
        1. G√©n√®re une requ√™te de recherche optimis√©e
        2. Effectue la recherche et g√©n√®re la r√©ponse
        """
        logger.info(f"\nüß† RECHERCHE INTELLIGENTE")
        logger.info(f"üìù Question utilisateur: {user_query}")
        
        try:
            # √âtape 1: G√©n√©rer la requ√™te de recherche optimale
            search_query_data = self._generate_search_query(
                user_query, 
                time_constraint, 
                current_date
            )
            
            if not search_query_data.get('search_query'):
                logger.warning("‚ùå Pas de requ√™te de recherche g√©n√©r√©e")
                return {
                    'response': "Je n'ai pas pu comprendre votre demande. Pouvez-vous reformuler?",
                    'sources': [],
                    'search_query': None
                }
            
            search_query = search_query_data['search_query']
            search_type = search_query_data.get('search_type', 'news')
            
            logger.info(f"üîç Requ√™te optimis√©e: \"{search_query}\"")
            logger.info(f"üìä Type de recherche: {search_type}")
            
            # √âtape 2: Effectuer la recherche
            search_results = self._perform_smart_search(
                search_query, 
                search_type,
                time_constraint,
                current_date
            )
            
            # √âtape 3: G√©n√©rer la r√©ponse finale avec le contexte
            final_response = self._generate_final_response(
                user_query,
                search_results,
                search_query,
                current_date,
                time_constraint
            )
            
            # Pr√©parer les sources
            sources = []
            if search_results:
                sources = [
                    {
                        'title': r.get('title', ''),
                        'url': r.get('url', ''),
                        'date': r.get('date', ''),
                        'relevance_score': r.get('relevance_score', 0.5)
                    }
                    for r in search_results[:5]  # Top 5 sources
                ]
            
            return {
                'response': final_response,
                'sources': sources,
                'search_query': search_query,
                'search_type': search_type
            }
            
        except Exception as e:
            logger.error(f"Erreur recherche intelligente: {str(e)}", exc_info=True)
            return {
                'response': "Une erreur s'est produite lors du traitement de votre demande.",
                'sources': [],
                'error': str(e)
            }
    
    def _generate_search_query(
        self,
        user_query: str,
        time_constraint: Optional[str],
        current_date: Optional[datetime]
    ) -> Dict[str, str]:
        """
        Utilise le LLM pour g√©n√©rer une requ√™te de recherche optimale
        """
        # Informations temporelles
        date_info = ""
        if current_date:
            date_info = f"\nDate actuelle: {current_date.strftime('%d/%m/%Y')} (Semaine {current_date.isocalendar()[1]})"
            if time_constraint:
                date_info += f"\nContrainte temporelle: {time_constraint}"
        
        # Prompt pour g√©n√©rer la requ√™te
        system_prompt = f"""Tu es un expert en recherche web. Ta t√¢che est d'analyser la question de l'utilisateur et de g√©n√©rer LA MEILLEURE requ√™te de recherche possible pour obtenir des informations pertinentes et actuelles.
{date_info}

INSTRUCTIONS:
1. Analyse la question pour identifier les concepts cl√©s
2. D√©termine s'il s'agit d'une recherche d'actualit√©s, technique, ou g√©n√©rale
3. G√©n√®re une requ√™te de recherche OPTIMIS√âE qui maximisera la pertinence des r√©sultats
4. Pour les actualit√©s, ajoute des mots-cl√©s temporels pertinents (2025, latest, announced, etc.)
5. Pour les sujets techniques, ajoute des termes sp√©cifiques
6. Utilise des op√©rateurs de recherche si n√©cessaire (OR, "guillemets", etc.)

IMPORTANT: 
- La requ√™te doit √™tre EN ANGLAIS pour de meilleurs r√©sultats
- Elle doit √™tre concise mais pr√©cise
- Pour l'IA g√©n√©rative, privil√©gie les noms d'entreprises et de mod√®les sp√©cifiques
- Pour les actualit√©s r√©centes, ajoute TOUJOURS des termes temporels (today, this week, latest, announced)
- Inclus des noms de soci√©t√©s cl√©s: OpenAI, Anthropic, Google, Meta, Microsoft

R√©ponds UNIQUEMENT avec un JSON structur√©. Assure-toi que ta r√©ponse est un JSON valide et rien d'autre:
{{
  "search_query": "la requ√™te de recherche optimis√©e en anglais",
  "search_type": "news|technical|general",
  "keywords": ["mot1", "mot2", "mot3"],
  "reasoning": "explication courte de ta strat√©gie"
}}

IMPORTANT: Ta r√©ponse doit √™tre SEULEMENT le JSON, sans texte avant ou apr√®s."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Question de l'utilisateur: {user_query}"}
        ]
        
        try:
            if self.use_huggingface:
                # Utiliser Hugging Face Lite
                logger.info("\n" + "ü§ó"*20)
                logger.info("ü§ó UTILISATION DE HUGGING FACE (LLM LOCAL)")
                logger.info("ü§ó"*20)
                response_text = self.llm_service.generate_search_query(messages)
                
                # Parser le JSON de la r√©ponse
                try:
                    response_text = response_text.strip()
                    if '{' in response_text and '}' in response_text:
                        start = response_text.find('{')
                        end = response_text.rfind('}') + 1
                        json_str = response_text[start:end]
                        search_data = json.loads(json_str)
                        logger.info(f"‚úÖ Requ√™te g√©n√©r√©e par HF: {search_data.get('search_query', '')}")
                        return search_data
                    else:
                        raise json.JSONDecodeError("No JSON found", response_text, 0)
                except json.JSONDecodeError:
                    logger.warning("‚ùå Pas de JSON valide de HF, utilisation du fallback")
                    return {
                        'search_query': self._extract_query_from_text(user_query),
                        'search_type': 'general'
                    }
            else:
                # Fallback vers OpenRouter
                logger.info("\n" + "üåê"*20)
                logger.info("üåê UTILISATION D'OPENROUTER (API EXTERNE)")
                logger.info(f"ü§ñ Mod√®le: {self.model}")
                logger.info("üåê"*20)
                response = httpx.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json={
                        "model": self.model,
                        "messages": messages,
                        "temperature": 0.3,
                        "max_tokens": 200
                    },
                    timeout=15.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    
                    # Parser le JSON
                    try:
                        # Nettoyer le contenu au cas o√π il y aurait du texte autour
                        content = content.strip()
                        # Essayer de trouver le JSON dans la r√©ponse
                        if '{' in content and '}' in content:
                            start = content.find('{') 
                            end = content.rfind('}') + 1
                            json_str = content[start:end]
                            search_data = json.loads(json_str)
                            logger.info(f"‚úÖ JSON pars√© avec succ√®s")
                            return search_data
                        else:
                            raise json.JSONDecodeError("No JSON found", content, 0)
                    except json.JSONDecodeError as e:
                        # Fallback: extraire la requ√™te du texte
                        logger.warning(f"‚ùå Erreur parsing JSON: {e}")
                        logger.warning(f"‚ùå Contenu re√ßu: {content[:200]}...")
                        return {
                            'search_query': self._extract_query_from_text(user_query),
                            'search_type': 'general'
                        }
                else:
                    logger.error(f"Erreur API: {response.status_code}")
                    logger.error(f"R√©ponse: {response.text}")
                    logger.error(f"Mod√®le utilis√©: {self.model}")
                    # Utiliser la requ√™te originale en cas d'erreur
                    return {'search_query': user_query, 'search_type': 'general'}
                
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration requ√™te: {e}")
            # Fallback: utiliser la requ√™te originale
            return {'search_query': user_query, 'search_type': 'general'}
    
    def _extract_query_from_text(self, text: str) -> str:
        """Extrait une requ√™te de recherche optimis√©e du texte"""
        text_lower = text.lower()
        
        # Traductions et mots-cl√©s
        translations = {
            'ia g√©n√©rative': 'generative AI',
            'intelligence artificielle': 'AI artificial intelligence',
            'd√©veloppements': 'developments',
            'annonc√©s': 'announced',
            'cette semaine': 'this week',
            'derniers': 'latest',
            'r√©cents': 'recent',
            'nouveaux': 'new',
            'exemples': 'examples'
        }
        
        # Remplacer les termes fran√ßais par anglais
        query = text_lower
        for fr, en in translations.items():
            query = query.replace(fr, en)
        
        # Supprimer les mots interrogatifs
        remove_words = [
            'quels', 'sont', 'les', 'qu\'est-ce', 'que', 'comment',
            'pourquoi', 'o√π', 'quand', 'quel', 'quelle', 'donne-moi',
            'avec', 'leurs', 'sources', 'concrets'
        ]
        
        words = query.split()
        filtered = [w for w in words if w not in remove_words]
        
        # Ajouter l'ann√©e actuelle et des mots-cl√©s pertinents
        filtered.extend(['2025', 'news', 'latest', 'announced', 'today'])
        
        # Ajouter des noms d'entreprises pour l'IA
        if 'generative' in query or 'ai' in query:
            filtered.extend(['OpenAI', 'Anthropic', 'Google', 'Meta', 'Microsoft'])
        
        # Construire la requ√™te finale
        final_query = ' '.join(filtered)
        logger.info(f"üîÑ Requ√™te extraite: {final_query}")
        
        return final_query
    
    def _perform_smart_search(
        self,
        search_query: str,
        search_type: str,
        time_constraint: Optional[str],
        current_date: Optional[datetime]
    ) -> List[Dict]:
        """
        Effectue la recherche avec la requ√™te optimis√©e
        """
        logger.info(f"\nüîé Recherche en cours: \"{search_query}\"")
        
        try:
            # Utiliser SerpAPI en priorit√© SANS cache pour les nouvelles recherches
            results = self.serpapi_service.search(
                query=search_query,
                search_type=search_type,
                use_cache=False  # Forcer une nouvelle recherche
            )
            
            # Si pas de r√©sultats, essayer MultiSearch
            if not results:
                logger.warning("‚ö†Ô∏è Pas de r√©sultats SerpAPI, essai MultiSearch")
                results = self.multi_search.search(search_query)
            
            # Filtrer par date si n√©cessaire
            # MAIS garder les r√©sultats sans date si on n'a rien de mieux
            if time_constraint and results:
                filtered_results = self._filter_by_date(
                    results,
                    time_constraint,
                    current_date
                )
                # Si le filtrage supprime tout, garder les r√©sultats originaux
                if not filtered_results and results:
                    logger.warning("‚ö†Ô∏è Aucun r√©sultat avec date r√©cente, utilisation des r√©sultats sans date")
                    results = results[:5]  # Garder les 5 premiers
                else:
                    results = filtered_results
            
            logger.info(f"‚úÖ {len(results)} r√©sultats trouv√©s")
            return results
            
        except Exception as e:
            logger.error(f"Erreur recherche: {e}")
            return []
    
    def _filter_by_date(
        self,
        results: List[Dict],
        time_constraint: str,
        current_date: Optional[datetime]
    ) -> List[Dict]:
        """Filtre les r√©sultats selon la contrainte temporelle"""
        if not current_date:
            current_date = datetime.now()
        
        # D√©finir la p√©riode
        if time_constraint == 'this_week':
            start_date = current_date - timedelta(days=current_date.weekday())
            end_date = current_date + timedelta(days=1)
        elif time_constraint == 'today':
            start_date = current_date.replace(hour=0, minute=0, second=0)
            end_date = current_date + timedelta(days=1)
        elif time_constraint == 'recent':
            start_date = current_date - timedelta(days=7)
            end_date = current_date + timedelta(days=1)
        else:
            return results
        
        filtered = []
        for result in results:
            # Utiliser date_parsed si disponible
            if result.get('date_parsed'):
                try:
                    if isinstance(result['date_parsed'], str):
                        result_date = datetime.fromisoformat(result['date_parsed'])
                    else:
                        result_date = result['date_parsed']
                    
                    if start_date <= result_date <= end_date:
                        filtered.append(result)
                except:
                    # Inclure avec score r√©duit si erreur
                    result['relevance_score'] = result.get('relevance_score', 0.5) * 0.7
                    filtered.append(result)
            else:
                # Pas de date, inclure selon le type
                if time_constraint not in ['this_week', 'today']:
                    result['relevance_score'] = result.get('relevance_score', 0.5) * 0.5
                    filtered.append(result)
        
        return filtered
    
    def _generate_final_response(
        self,
        user_query: str,
        search_results: List[Dict],
        search_query: str,
        current_date: Optional[datetime],
        time_constraint: Optional[str]
    ) -> str:
        """
        G√©n√®re la r√©ponse finale en utilisant les r√©sultats de recherche
        """
        if not search_results:
            return f"Je n'ai pas trouv√© d'informations r√©centes pour votre recherche \"{search_query}\". Essayez de reformuler votre question ou de pr√©ciser ce que vous cherchez."
        
        # Formater le contexte
        context = self._format_search_context(search_results)
        
        # Informations temporelles
        date_info = ""
        if current_date:
            date_info = f"""
üìÖ DATE ACTUELLE: {current_date.strftime('%d/%m/%Y')}
üìÖ SEMAINE: {current_date.isocalendar()[1]} de {current_date.year}
‚è∞ P√âRIODE DEMAND√âE: {time_constraint or 'Non sp√©cifi√©e'}
üîç REQU√äTE DE RECHERCHE UTILIS√âE: "{search_query}"
"""
        
        # Prompt pour la r√©ponse finale
        system_prompt = f"""Tu es un assistant IA expert qui r√©pond aux questions en utilisant EXCLUSIVEMENT les informations des r√©sultats de recherche fournis.
{date_info}

üî¥ R√àGLES ABSOLUES:
1. Tu DOIS baser ta r√©ponse UNIQUEMENT sur le contexte ci-dessous
2. Tu DOIS citer chaque information avec [Source: Titre de l'article]
3. Si une information n'est pas dans le contexte, dis "Cette information n'est pas disponible dans les sources trouv√©es"
4. Structure ta r√©ponse de mani√®re claire et organis√©e
5. Termine TOUJOURS par une section "üìö Sources consult√©es:" avec les titres et URLs

üì∞ R√âSULTATS DE RECHERCHE (UTILISE UNIQUEMENT CES INFORMATIONS):
{context}

‚ö†Ô∏è NE PAS inventer ou utiliser des connaissances non pr√©sentes dans le contexte ci-dessus."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ]
        
        try:
            if self.use_huggingface:
                # Utiliser Hugging Face pour la r√©ponse finale
                logger.info("\n" + "ü§ó"*20)
                logger.info("ü§ó G√âN√âRATION FINALE AVEC HUGGING FACE (LLM LOCAL)")
                logger.info("ü§ó +5 POINTS BONUS!")
                logger.info("ü§ó"*20)
                return self.llm_service.generate_response(
                    query=user_query,
                    search_results=search_results,
                    current_date=current_date,
                    time_constraint=time_constraint
                )
            else:
                # Fallback vers OpenRouter
                logger.info("\n" + "üåê"*20)
                logger.info("üåê G√âN√âRATION FINALE AVEC OPENROUTER (API EXTERNE)")
                logger.info(f"ü§ñ Mod√®le: {self.model}")
                logger.info("üåê"*20)
                response = httpx.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json={
                        "model": self.model,
                        "messages": messages,
                        "temperature": 0.3,
                        "max_tokens": 2000
                    },
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result['choices'][0]['message']['content']
                else:
                    logger.error(f"Erreur g√©n√©ration r√©ponse: {response.status_code}")
                    return "Erreur lors de la g√©n√©ration de la r√©ponse."
                
        except Exception as e:
            logger.error(f"Erreur r√©ponse finale: {e}")
            return "Une erreur s'est produite lors de la g√©n√©ration de la r√©ponse."
    
    def _format_search_context(self, search_results: List[Dict]) -> str:
        """Formate les r√©sultats pour le contexte"""
        formatted = []
        
        for i, result in enumerate(search_results[:10], 1):  # Max 10 r√©sultats
            title = result.get('title', 'Sans titre')
            url = result.get('url', '#')
            content = result.get('content', 'Contenu non disponible')
            date = result.get('date', 'Date non sp√©cifi√©e')
            source = result.get('source', 'Source inconnue')
            
            formatted_result = f"""
=== R√âSULTAT {i} ===
üì∞ TITRE: {title}
üåê SOURCE: {source}
üìÖ DATE DE PUBLICATION: {date}
üîó URL: {url}
üìù CONTENU:
{content}
{'='*60}"""
            formatted.append(formatted_result)
        
        return "\n".join(formatted)
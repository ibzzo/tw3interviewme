"""
Service de recherche intelligent avec gÃ©nÃ©ration de requÃªte par LLM
"""
import httpx
import json
import logging
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
from django.conf import settings

from .serpapi_service import SerpAPIService
from .multi_search import MultiSearchService
try:
    from .huggingface_lite import HuggingFaceLiteService
except ImportError:
    HuggingFaceLiteService = None

logger = logging.getLogger(__name__)


class IntelligentSearchService:
    """Service de recherche intelligent qui utilise le LLM pour optimiser les requÃªtes"""
    
    def __init__(self):
        # On n'utilise plus OpenRouter pour le LLM principal
        # Services de recherche
        self.serpapi_service = SerpAPIService()
        self.multi_search = MultiSearchService()
        
        # Service LLM avec Hugging Face Lite
        logger.info("\nğŸ§  CONFIGURATION DU SERVICE LLM")
        logger.info("-" * 40)
        
        self.use_huggingface = False
        if HuggingFaceLiteService:
            logger.info("ğŸ” Tentative d'utilisation de Hugging Face...")
            try:
                self.llm_service = HuggingFaceLiteService()
                if self.llm_service.is_available():
                    self.use_huggingface = True
                    logger.info("âœ… SERVICE SÃ‰LECTIONNÃ‰: Hugging Face (LLM Local)")
                    logger.info("ğŸ¯ Vous gagnez +5 points bonus pour l'utilisation d'un LLM local!")
                else:
                    logger.info("âš ï¸ Hugging Face non disponible")
                    logger.info("ğŸ”„ Basculement vers OpenRouter")
            except Exception as e:
                logger.error(f"âŒ Erreur initialisation Hugging Face: {e}")
                logger.info("ğŸ”„ Basculement vers OpenRouter")
        else:
            logger.info("âŒ Module HuggingFaceLiteService non trouvÃ©")
        
        # Configuration OpenRouter comme fallback
        if not self.use_huggingface:
            logger.info("ğŸŒ SERVICE SÃ‰LECTIONNÃ‰: OpenRouter (API Externe)")
            logger.info("ğŸ’¡ Pour utiliser un LLM local (+5 points bonus):")
            logger.info("   pip install transformers torch accelerate")
            self.api_key = settings.OPENROUTER_API_KEY
            self.base_url = settings.OPENROUTER_BASE_URL
            self.model = settings.OPENROUTER_MODEL
            self.headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "http://localhost:3000",
                "X-Title": "AI Chatbot Intelligent Search"
            }
    
    def process_user_query(
        self,
        user_query: str,
        time_constraint: Optional[str] = None,
        current_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """
        Traite la requÃªte utilisateur en 2 Ã©tapes :
        1. GÃ©nÃ¨re une requÃªte de recherche optimisÃ©e
        2. Effectue la recherche et gÃ©nÃ¨re la rÃ©ponse
        """
        logger.info(f"\nğŸ§  RECHERCHE INTELLIGENTE")
        logger.info(f"ğŸ“ Question utilisateur: {user_query}")
        
        try:
            # Ã‰tape 1: GÃ©nÃ©rer la requÃªte de recherche optimale
            search_query_data = self._generate_search_query(
                user_query, 
                time_constraint, 
                current_date
            )
            
            if not search_query_data.get('search_query'):
                logger.warning("âŒ Pas de requÃªte de recherche gÃ©nÃ©rÃ©e")
                return {
                    'response': "Je n'ai pas pu comprendre votre demande. Pouvez-vous reformuler?",
                    'sources': [],
                    'search_query': None
                }
            
            search_query = search_query_data['search_query']
            search_type = search_query_data.get('search_type', 'news')
            
            logger.info(f"ğŸ” RequÃªte optimisÃ©e: \"{search_query}\"")
            logger.info(f"ğŸ“Š Type de recherche: {search_type}")
            
            # Ã‰tape 2: Effectuer la recherche
            search_results = self._perform_smart_search(
                search_query, 
                search_type,
                time_constraint,
                current_date
            )
            
            # Ã‰tape 3: GÃ©nÃ©rer la rÃ©ponse finale avec le contexte
            final_response = self._generate_final_response(
                user_query,
                search_results,
                search_query,
                current_date,
                time_constraint
            )
            
            # PrÃ©parer les sources
            sources = []
            if search_results:
                sources = [
                    {
                        'title': r.get('title', ''),
                        'url': r.get('url', ''),
                        'date': r.get('date', ''),
                        'relevance_score': r.get('relevance_score', 0.5)
                    }
                    for r in search_results[:5]  # Top 5 sources
                ]
            
            return {
                'response': final_response,
                'sources': sources,
                'search_query': search_query,
                'search_type': search_type
            }
            
        except Exception as e:
            logger.error(f"Erreur recherche intelligente: {str(e)}", exc_info=True)
            return {
                'response': "Une erreur s'est produite lors du traitement de votre demande.",
                'sources': [],
                'error': str(e)
            }
    
    def _generate_search_query(
        self,
        user_query: str,
        time_constraint: Optional[str],
        current_date: Optional[datetime]
    ) -> Dict[str, str]:
        """
        Utilise le LLM pour gÃ©nÃ©rer une requÃªte de recherche optimale
        """
        # Informations temporelles
        date_info = ""
        if current_date:
            date_info = f"\nDate actuelle: {current_date.strftime('%d/%m/%Y')} (Semaine {current_date.isocalendar()[1]})"
            if time_constraint:
                date_info += f"\nContrainte temporelle: {time_constraint}"
        
        # Prompt pour gÃ©nÃ©rer la requÃªte
        system_prompt = f"""Tu es un expert en recherche web. Ta tÃ¢che est d'analyser la question de l'utilisateur et de gÃ©nÃ©rer LA MEILLEURE requÃªte de recherche possible pour obtenir des informations pertinentes et actuelles.
{date_info}

INSTRUCTIONS:
1. Analyse la question pour identifier les concepts clÃ©s
2. DÃ©termine s'il s'agit d'une recherche d'actualitÃ©s, technique, ou gÃ©nÃ©rale
3. GÃ©nÃ¨re une requÃªte de recherche OPTIMISÃ‰E qui maximisera la pertinence des rÃ©sultats
4. Pour les actualitÃ©s, ajoute des mots-clÃ©s temporels pertinents (2025, latest, announced, etc.)
5. Pour les sujets techniques, ajoute des termes spÃ©cifiques
6. Utilise des opÃ©rateurs de recherche si nÃ©cessaire (OR, "guillemets", etc.)

IMPORTANT: 
- La requÃªte doit Ãªtre EN ANGLAIS pour de meilleurs rÃ©sultats
- Elle doit Ãªtre concise mais prÃ©cise
- Pour l'IA gÃ©nÃ©rative, privilÃ©gie les noms d'entreprises et de modÃ¨les spÃ©cifiques
- Pour les actualitÃ©s rÃ©centes, ajoute TOUJOURS des termes temporels (today, this week, latest, announced)
- Inclus des noms de sociÃ©tÃ©s clÃ©s: OpenAI, Anthropic, Google, Meta, Microsoft

RÃ©ponds UNIQUEMENT avec un JSON structurÃ©. Assure-toi que ta rÃ©ponse est un JSON valide et rien d'autre:
{{
  "search_query": "la requÃªte de recherche optimisÃ©e en anglais",
  "search_type": "news|technical|general",
  "keywords": ["mot1", "mot2", "mot3"],
  "reasoning": "explication courte de ta stratÃ©gie"
}}

IMPORTANT: Ta rÃ©ponse doit Ãªtre SEULEMENT le JSON, sans texte avant ou aprÃ¨s."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Question de l'utilisateur: {user_query}"}
        ]
        
        try:
            if self.use_huggingface:
                # Utiliser Hugging Face Lite
                logger.info("\n" + "ğŸ¤—"*20)
                logger.info("ğŸ¤— UTILISATION DE HUGGING FACE (LLM LOCAL)")
                logger.info("ğŸ¤—"*20)
                response_text = self.llm_service.generate_search_query(messages)
                
                # Parser le JSON de la rÃ©ponse
                try:
                    response_text = response_text.strip()
                    if '{' in response_text and '}' in response_text:
                        start = response_text.find('{')
                        end = response_text.rfind('}') + 1
                        json_str = response_text[start:end]
                        search_data = json.loads(json_str)
                        logger.info(f"âœ… RequÃªte gÃ©nÃ©rÃ©e par HF: {search_data.get('search_query', '')}")
                        return search_data
                    else:
                        raise json.JSONDecodeError("No JSON found", response_text, 0)
                except json.JSONDecodeError:
                    logger.warning("âŒ Pas de JSON valide de HF, utilisation du fallback")
                    return {
                        'search_query': self._extract_query_from_text(user_query),
                        'search_type': 'general'
                    }
            else:
                # Fallback vers OpenRouter
                logger.info("\n" + "ğŸŒ"*20)
                logger.info("ğŸŒ UTILISATION D'OPENROUTER (API EXTERNE)")
                logger.info(f"ğŸ¤– ModÃ¨le: {self.model}")
                logger.info("ğŸŒ"*20)
                response = httpx.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json={
                        "model": self.model,
                        "messages": messages,
                        "temperature": 0.3,
                        "max_tokens": 200
                    },
                    timeout=15.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    
                    # Parser le JSON
                    try:
                        # Nettoyer le contenu au cas oÃ¹ il y aurait du texte autour
                        content = content.strip()
                        # Essayer de trouver le JSON dans la rÃ©ponse
                        if '{' in content and '}' in content:
                            start = content.find('{') 
                            end = content.rfind('}') + 1
                            json_str = content[start:end]
                            search_data = json.loads(json_str)
                            logger.info(f"âœ… JSON parsÃ© avec succÃ¨s")
                            return search_data
                        else:
                            raise json.JSONDecodeError("No JSON found", content, 0)
                    except json.JSONDecodeError as e:
                        # Fallback: extraire la requÃªte du texte
                        logger.warning(f"âŒ Erreur parsing JSON: {e}")
                        logger.warning(f"âŒ Contenu reÃ§u: {content[:200]}...")
                        return {
                            'search_query': self._extract_query_from_text(user_query),
                            'search_type': 'general'
                        }
                else:
                    logger.error(f"Erreur API: {response.status_code}")
                    logger.error(f"RÃ©ponse: {response.text}")
                    logger.error(f"ModÃ¨le utilisÃ©: {self.model}")
                    # Utiliser la requÃªte originale en cas d'erreur
                    return {'search_query': user_query, 'search_type': 'general'}
                
        except Exception as e:
            logger.error(f"Erreur gÃ©nÃ©ration requÃªte: {e}")
            # Fallback: utiliser la requÃªte originale
            return {'search_query': user_query, 'search_type': 'general'}
    
    def _extract_query_from_text(self, text: str) -> str:
        """Extrait une requÃªte de recherche optimisÃ©e du texte"""
        text_lower = text.lower()
        
        # Traductions et mots-clÃ©s
        translations = {
            'ia gÃ©nÃ©rative': 'generative AI',
            'intelligence artificielle': 'AI artificial intelligence',
            'dÃ©veloppements': 'developments',
            'annoncÃ©s': 'announced',
            'cette semaine': 'this week',
            'derniers': 'latest',
            'rÃ©cents': 'recent',
            'nouveaux': 'new',
            'exemples': 'examples'
        }
        
        # Remplacer les termes franÃ§ais par anglais
        query = text_lower
        for fr, en in translations.items():
            query = query.replace(fr, en)
        
        # Supprimer les mots interrogatifs
        remove_words = [
            'quels', 'sont', 'les', 'qu\'est-ce', 'que', 'comment',
            'pourquoi', 'oÃ¹', 'quand', 'quel', 'quelle', 'donne-moi',
            'avec', 'leurs', 'sources', 'concrets'
        ]
        
        words = query.split()
        filtered = [w for w in words if w not in remove_words]
        
        # Ajouter l'annÃ©e actuelle et des mots-clÃ©s pertinents
        filtered.extend(['2025', 'news', 'latest', 'announced', 'today'])
        
        # Ajouter des noms d'entreprises pour l'IA
        if 'generative' in query or 'ai' in query:
            filtered.extend(['OpenAI', 'Anthropic', 'Google', 'Meta', 'Microsoft'])
        
        # Construire la requÃªte finale
        final_query = ' '.join(filtered)
        logger.info(f"ğŸ”„ RequÃªte extraite: {final_query}")
        
        return final_query
    
    def _perform_smart_search(
        self,
        search_query: str,
        search_type: str,
        time_constraint: Optional[str],
        current_date: Optional[datetime]
    ) -> List[Dict]:
        """
        Effectue la recherche avec la requÃªte optimisÃ©e
        """
        logger.info(f"\nğŸ” Recherche en cours: \"{search_query}\"")
        
        try:
            # Utiliser SerpAPI en prioritÃ© SANS cache pour les nouvelles recherches
            results = self.serpapi_service.search(
                query=search_query,
                search_type=search_type,
                use_cache=False  # Forcer une nouvelle recherche
            )
            
            # Si pas de rÃ©sultats, essayer MultiSearch
            if not results:
                logger.warning("âš ï¸ Pas de rÃ©sultats SerpAPI, essai MultiSearch")
                results = self.multi_search.search(search_query)
            
            # Filtrer par date si nÃ©cessaire
            # MAIS garder les rÃ©sultats sans date si on n'a rien de mieux
            if time_constraint and results:
                filtered_results = self._filter_by_date(
                    results,
                    time_constraint,
                    current_date
                )
                # Si le filtrage supprime tout, garder les rÃ©sultats originaux
                if not filtered_results and results:
                    logger.warning("âš ï¸ Aucun rÃ©sultat avec date rÃ©cente, utilisation des rÃ©sultats sans date")
                    results = results[:5]  # Garder les 5 premiers
                else:
                    results = filtered_results
            
            logger.info(f"âœ… {len(results)} rÃ©sultats trouvÃ©s")
            return results
            
        except Exception as e:
            logger.error(f"Erreur recherche: {e}")
            return []
    
    def _filter_by_date(
        self,
        results: List[Dict],
        time_constraint: str,
        current_date: Optional[datetime]
    ) -> List[Dict]:
        """Filtre les rÃ©sultats selon la contrainte temporelle"""
        if not current_date:
            current_date = datetime.now()
        
        # DÃ©finir la pÃ©riode
        if time_constraint == 'this_week':
            start_date = current_date - timedelta(days=current_date.weekday())
            end_date = current_date + timedelta(days=1)
        elif time_constraint == 'today':
            start_date = current_date.replace(hour=0, minute=0, second=0)
            end_date = current_date + timedelta(days=1)
        elif time_constraint == 'recent':
            start_date = current_date - timedelta(days=7)
            end_date = current_date + timedelta(days=1)
        else:
            return results
        
        filtered = []
        for result in results:
            # Utiliser date_parsed si disponible
            if result.get('date_parsed'):
                try:
                    if isinstance(result['date_parsed'], str):
                        result_date = datetime.fromisoformat(result['date_parsed'])
                    else:
                        result_date = result['date_parsed']
                    
                    if start_date <= result_date <= end_date:
                        filtered.append(result)
                except:
                    # Inclure avec score rÃ©duit si erreur
                    result['relevance_score'] = result.get('relevance_score', 0.5) * 0.7
                    filtered.append(result)
            else:
                # Pas de date, inclure selon le type
                if time_constraint not in ['this_week', 'today']:
                    result['relevance_score'] = result.get('relevance_score', 0.5) * 0.5
                    filtered.append(result)
        
        return filtered
    
    def _generate_final_response(
        self,
        user_query: str,
        search_results: List[Dict],
        search_query: str,
        current_date: Optional[datetime],
        time_constraint: Optional[str]
    ) -> str:
        """
        GÃ©nÃ¨re la rÃ©ponse finale en utilisant les rÃ©sultats de recherche
        """
        if not search_results:
            return f"Je n'ai pas trouvÃ© d'informations rÃ©centes pour votre recherche \"{search_query}\". Essayez de reformuler votre question ou de prÃ©ciser ce que vous cherchez."
        
        # Formater le contexte
        context = self._format_search_context(search_results)
        
        # Informations temporelles
        date_info = ""
        if current_date:
            date_info = f"""
ğŸ“… DATE ACTUELLE: {current_date.strftime('%d/%m/%Y')}
ğŸ“… SEMAINE: {current_date.isocalendar()[1]} de {current_date.year}
â° PÃ‰RIODE DEMANDÃ‰E: {time_constraint or 'Non spÃ©cifiÃ©e'}
ğŸ” REQUÃŠTE DE RECHERCHE UTILISÃ‰E: "{search_query}"
"""
        
        # Prompt pour la rÃ©ponse finale
        system_prompt = f"""Tu es un assistant IA expert qui rÃ©pond aux questions en utilisant EXCLUSIVEMENT les informations des rÃ©sultats de recherche fournis.
{date_info}

ğŸ”´ RÃˆGLES ABSOLUES:
1. Tu DOIS baser ta rÃ©ponse UNIQUEMENT sur le contexte ci-dessous
2. Tu DOIS citer chaque information avec [Source: Titre de l'article]
3. Si une information n'est pas dans le contexte, dis "Cette information n'est pas disponible dans les sources trouvÃ©es"
4. Structure ta rÃ©ponse de maniÃ¨re claire et organisÃ©e
5. Termine TOUJOURS par une section "ğŸ“š Sources consultÃ©es:" avec les titres et URLs

ğŸ“° RÃ‰SULTATS DE RECHERCHE (UTILISE UNIQUEMENT CES INFORMATIONS):
{context}

âš ï¸ NE PAS inventer ou utiliser des connaissances non prÃ©sentes dans le contexte ci-dessus."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ]
        
        try:
            if self.use_huggingface:
                # Utiliser Hugging Face pour la rÃ©ponse finale
                logger.info("\n" + "ğŸ¤—"*20)
                logger.info("ğŸ¤— GÃ‰NÃ‰RATION FINALE AVEC HUGGING FACE (LLM LOCAL)")
                logger.info("ğŸ¤— +5 POINTS BONUS!")
                logger.info("ğŸ¤—"*20)
                return self.llm_service.generate_response(
                    query=user_query,
                    search_results=search_results,
                    current_date=current_date,
                    time_constraint=time_constraint
                )
            else:
                # Fallback vers OpenRouter
                logger.info("\n" + "ğŸŒ"*20)
                logger.info("ğŸŒ GÃ‰NÃ‰RATION FINALE AVEC OPENROUTER (API EXTERNE)")
                logger.info(f"ğŸ¤– ModÃ¨le: {self.model}")
                logger.info("ğŸŒ"*20)
                response = httpx.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json={
                        "model": self.model,
                        "messages": messages,
                        "temperature": 0.3,
                        "max_tokens": 2000
                    },
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result['choices'][0]['message']['content']
                else:
                    logger.error(f"Erreur gÃ©nÃ©ration rÃ©ponse: {response.status_code}")
                    return "Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse."
                
        except Exception as e:
            logger.error(f"Erreur rÃ©ponse finale: {e}")
            return "Une erreur s'est produite lors de la gÃ©nÃ©ration de la rÃ©ponse."
    
    def _format_search_context(self, search_results: List[Dict]) -> str:
        """Formate les rÃ©sultats pour le contexte"""
        formatted = []
        
        for i, result in enumerate(search_results[:10], 1):  # Max 10 rÃ©sultats
            title = result.get('title', 'Sans titre')
            url = result.get('url', '#')
            content = result.get('content', 'Contenu non disponible')
            date = result.get('date', 'Date non spÃ©cifiÃ©e')
            source = result.get('source', 'Source inconnue')
            
            formatted_result = f"""
=== RÃ‰SULTAT {i} ===
ğŸ“° TITRE: {title}
ğŸŒ SOURCE: {source}
ğŸ“… DATE DE PUBLICATION: {date}
ğŸ”— URL: {url}
ğŸ“ CONTENU:
{content}
{'='*60}"""
            formatted.append(formatted_result)
        
        return "\n".join(formatted)
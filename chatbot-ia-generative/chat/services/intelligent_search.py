"""
Service de recherche intelligent avec gÃ©nÃ©ration de requÃªte par LLM
"""
import httpx
import json
import logging
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
from django.conf import settings

from .serpapi_service import SerpAPIService
from .multi_search import MultiSearchService
from .vllm_service import VLLMService
from .openrouter_optimized import OpenRouterOptimizedService
from django.core.cache import cache

logger = logging.getLogger(__name__)


class IntelligentSearchService:
    """Service de recherche intelligent qui utilise le LLM pour optimiser les requÃªtes"""
    
    def __init__(self):
        # Services de recherche
        self.serpapi_service = SerpAPIService()
        self.multi_search = MultiSearchService()
        
        # Services LLM
        self.vllm_service = VLLMService()
        self.openrouter_service = OpenRouterOptimizedService()
        
        # Configuration OpenRouter pour les cas oÃ¹ on en a encore besoin
        self.api_key = settings.OPENROUTER_API_KEY
        self.base_url = settings.OPENROUTER_BASE_URL
        self.model = settings.OPENROUTER_MODEL
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "http://localhost:3000",
            "X-Title": "AI Chatbot Intelligent Search"
        }
    
    def process_user_query(
        self,
        user_query: str,
        time_constraint: Optional[str] = None,
        current_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """
        Traite la requÃªte utilisateur en 2 Ã©tapes :
        1. GÃ©nÃ¨re une requÃªte de recherche optimisÃ©e
        2. Effectue la recherche et gÃ©nÃ¨re la rÃ©ponse
        """
        
        try:
            # Ã‰tape 1: GÃ©nÃ©rer la requÃªte de recherche optimale
            search_query_data = self._generate_search_query(
                user_query, 
                time_constraint, 
                current_date
            )
            
            if not search_query_data.get('search_query'):
                logger.warning("âŒ Pas de requÃªte de recherche gÃ©nÃ©rÃ©e")
                return {
                    'response': "Je n'ai pas pu comprendre votre demande. Pouvez-vous reformuler?",
                    'sources': [],
                    'search_query': None
                }
            
            search_query = search_query_data['search_query']
            search_type = search_query_data.get('search_type', 'news')
            
            
            # Ã‰tape 2: Effectuer la recherche
            search_results = self._perform_smart_search(
                search_query, 
                search_type,
                time_constraint,
                current_date
            )
            
            # Ã‰tape 3: GÃ©nÃ©rer la rÃ©ponse finale avec le contexte
            final_response = self._generate_final_response(
                user_query,
                search_results,
                search_query,
                current_date,
                time_constraint
            )
            
            # PrÃ©parer les sources
            sources = []
            if search_results:
                sources = [
                    {
                        'title': r.get('title', ''),
                        'url': r.get('url', ''),
                        'date': r.get('date', ''),
                        'relevance_score': r.get('relevance_score', 0.5)
                    }
                    for r in search_results[:5]  # Top 5 sources
                ]
            
            return {
                'response': final_response,
                'sources': sources,
                'search_query': search_query,
                'search_type': search_type
            }
            
        except Exception as e:
            logger.error(f"Erreur recherche intelligente: {str(e)}", exc_info=True)
            return {
                'response': "Une erreur s'est produite lors du traitement de votre demande.",
                'sources': [],
                'error': str(e)
            }
    
    def _generate_search_query(
        self,
        user_query: str,
        time_constraint: Optional[str],
        current_date: Optional[datetime]
    ) -> Dict[str, str]:
        """
        Utilise le LLM pour gÃ©nÃ©rer une requÃªte de recherche optimale
        """
        # Informations temporelles
        date_info = ""
        if current_date:
            date_info = f"\nDate actuelle: {current_date.strftime('%d/%m/%Y')} (Semaine {current_date.isocalendar()[1]})"
            if time_constraint:
                date_info += f"\nContrainte temporelle: {time_constraint}"
        
        # Prompt pour gÃ©nÃ©rer la requÃªte
        system_prompt = f"""Tu es un expert en recherche web. Ta tÃ¢che est d'analyser la question de l'utilisateur et de gÃ©nÃ©rer LA MEILLEURE requÃªte de recherche possible pour obtenir des informations pertinentes et actuelles.
{date_info}

INSTRUCTIONS:
1. Analyse la question pour identifier les concepts clÃ©s
2. DÃ©termine s'il s'agit d'une recherche d'actualitÃ©s, technique, ou gÃ©nÃ©rale
3. GÃ©nÃ¨re une requÃªte de recherche OPTIMISÃ‰E qui maximisera la pertinence des rÃ©sultats
4. Pour les actualitÃ©s, ajoute des mots-clÃ©s temporels pertinents (2025, latest, announced, etc.)
5. Pour les sujets techniques, ajoute des termes spÃ©cifiques
6. Utilise des opÃ©rateurs de recherche si nÃ©cessaire (OR, "guillemets", etc.)

IMPORTANT: 
- La requÃªte doit Ãªtre EN ANGLAIS pour de meilleurs rÃ©sultats
- Elle doit Ãªtre concise mais prÃ©cise
- Pour l'IA gÃ©nÃ©rative, privilÃ©gie les noms d'entreprises et de modÃ¨les spÃ©cifiques
- Pour les actualitÃ©s rÃ©centes, ajoute TOUJOURS des termes temporels (today, this week, latest, announced)
- Inclus des noms de sociÃ©tÃ©s clÃ©s: OpenAI, Anthropic, Google, Meta, Microsoft

RÃ©ponds UNIQUEMENT avec un JSON structurÃ©. Assure-toi que ta rÃ©ponse est un JSON valide et rien d'autre:
{{
  "search_query": "la requÃªte de recherche optimisÃ©e en anglais",
  "search_type": "news|technical|general",
  "keywords": ["mot1", "mot2", "mot3"],
  "reasoning": "explication courte de ta stratÃ©gie"
}}

IMPORTANT: Ta rÃ©ponse doit Ãªtre SEULEMENT le JSON, sans texte avant ou aprÃ¨s."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Question de l'utilisateur: {user_query}"}
        ]
        
        try:
            # Utiliser le modÃ¨le sÃ©lectionnÃ© dans le cache
            selected_model = cache.get('selected_llm_model', 'vllm')
            logger.info(f"ğŸ” GÃ©nÃ©ration requÃªte recherche avec: {selected_model}")
            
            if selected_model == 'vllm' and self.vllm_service.is_available():
                # Utiliser vLLM
                prompt = f"{system_prompt}\n\nUser: {messages[1]['content']}"
                response = self.vllm_service.generate_response(prompt=prompt)
                if response['success']:
                    response_text = response['response']
                    try:
                        # Parser le JSON
                        response_text = response_text.strip()
                        if '{' in response_text and '}' in response_text:
                            start = response_text.find('{')
                            end = response_text.rfind('}') + 1
                            json_str = response_text[start:end]
                            search_data = json.loads(json_str)
                            return search_data
                    except json.JSONDecodeError:
                        logger.warning("âŒ JSON invalide de vLLM, fallback")
                        return {
                            'search_query': self._extract_query_from_text(user_query),
                            'search_type': 'general'
                        }
                else:
                    selected_model = 'openrouter'
            
            # Si OpenRouter ou si vLLM a Ã©chouÃ©
            if selected_model == 'openrouter':
                response = httpx.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json={
                        "model": self.model,
                        "messages": messages,
                        "temperature": 0.3,
                        "max_tokens": 200
                    },
                    timeout=15.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    
                    # Parser le JSON
                    try:
                        # Nettoyer le contenu au cas oÃ¹ il y aurait du texte autour
                        content = content.strip()
                        # Essayer de trouver le JSON dans la rÃ©ponse
                        if '{' in content and '}' in content:
                            start = content.find('{') 
                            end = content.rfind('}') + 1
                            json_str = content[start:end]
                            search_data = json.loads(json_str)
                            return search_data
                        else:
                            raise json.JSONDecodeError("No JSON found", content, 0)
                    except json.JSONDecodeError as e:
                        # Fallback: extraire la requÃªte du texte
                        logger.warning(f"âŒ Erreur parsing JSON: {e}")
                        logger.warning(f"âŒ Contenu reÃ§u: {content[:200]}...")
                        return {
                            'search_query': self._extract_query_from_text(user_query),
                            'search_type': 'general'
                        }
                elif response.status_code == 429:
                    logger.warning("âš ï¸ Limite OpenRouter atteinte (429) - Utilisation requÃªte basique")
                    return {
                        'search_query': self._extract_query_from_text(user_query),
                        'search_type': 'general'
                    }
                else:
                    logger.error(f"Erreur API: {response.status_code}")
                    # Utiliser la requÃªte originale en cas d'erreur
                    return {'search_query': user_query, 'search_type': 'general'}
                
        except Exception as e:
            logger.error(f"Erreur gÃ©nÃ©ration requÃªte: {e}")
            # Fallback: utiliser la requÃªte originale
            return {'search_query': user_query, 'search_type': 'general'}
    
    def _extract_query_from_text(self, text: str) -> str:
        """Extrait une requÃªte de recherche optimisÃ©e du texte"""
        text_lower = text.lower()
        
        # Traductions et mots-clÃ©s
        translations = {
            'ia gÃ©nÃ©rative': 'generative AI',
            'intelligence artificielle': 'AI artificial intelligence',
            'dÃ©veloppements': 'developments',
            'annoncÃ©s': 'announced',
            'cette semaine': 'this week',
            'derniers': 'latest',
            'rÃ©cents': 'recent',
            'nouveaux': 'new',
            'exemples': 'examples'
        }
        
        # Remplacer les termes franÃ§ais par anglais
        query = text_lower
        for fr, en in translations.items():
            query = query.replace(fr, en)
        
        # Supprimer les mots interrogatifs
        remove_words = [
            'quels', 'sont', 'les', 'qu\'est-ce', 'que', 'comment',
            'pourquoi', 'oÃ¹', 'quand', 'quel', 'quelle', 'donne-moi',
            'avec', 'leurs', 'sources', 'concrets'
        ]
        
        words = query.split()
        filtered = [w for w in words if w not in remove_words]
        
        # Ajouter l'annÃ©e actuelle et des mots-clÃ©s pertinents
        filtered.extend(['2025', 'news', 'latest', 'announced', 'today'])
        
        # Ajouter des noms d'entreprises pour l'IA
        if 'generative' in query or 'ai' in query:
            filtered.extend(['OpenAI', 'Anthropic', 'Google', 'Meta', 'Microsoft'])
        
        # Construire la requÃªte finale
        final_query = ' '.join(filtered)
        
        return final_query
    
    def _perform_smart_search(
        self,
        search_query: str,
        search_type: str,
        time_constraint: Optional[str],
        current_date: Optional[datetime]
    ) -> List[Dict]:
        """
        Effectue la recherche avec la requÃªte optimisÃ©e
        """
        
        try:
            # Utiliser SerpAPI en prioritÃ© SANS cache pour les nouvelles recherches
            results = self.serpapi_service.search(
                query=search_query,
                search_type=search_type,
                use_cache=False  # Forcer une nouvelle recherche
            )
            
            # Si pas de rÃ©sultats, essayer MultiSearch
            if not results:
                logger.warning("âš ï¸ Pas de rÃ©sultats SerpAPI, essai MultiSearch")
                results = self.multi_search.search(search_query)
            
            # Filtrer par date si nÃ©cessaire
            # MAIS garder les rÃ©sultats sans date si on n'a rien de mieux
            if time_constraint and results:
                filtered_results = self._filter_by_date(
                    results,
                    time_constraint,
                    current_date
                )
                # Si le filtrage supprime tout, garder les rÃ©sultats originaux
                if not filtered_results and results:
                    logger.warning("âš ï¸ Aucun rÃ©sultat avec date rÃ©cente, utilisation des rÃ©sultats sans date")
                    results = results[:5]  # Garder les 5 premiers
                else:
                    results = filtered_results
            
            return results
            
        except Exception as e:
            logger.error(f"Erreur recherche: {e}")
            return []
    
    def _filter_by_date(
        self,
        results: List[Dict],
        time_constraint: str,
        current_date: Optional[datetime]
    ) -> List[Dict]:
        """Filtre les rÃ©sultats selon la contrainte temporelle"""
        if not current_date:
            current_date = datetime.now()
        
        # DÃ©finir la pÃ©riode
        if time_constraint == 'this_week':
            start_date = current_date - timedelta(days=current_date.weekday())
            end_date = current_date + timedelta(days=1)
        elif time_constraint == 'today':
            start_date = current_date.replace(hour=0, minute=0, second=0)
            end_date = current_date + timedelta(days=1)
        elif time_constraint == 'recent':
            start_date = current_date - timedelta(days=7)
            end_date = current_date + timedelta(days=1)
        else:
            return results
        
        filtered = []
        for result in results:
            # Utiliser date_parsed si disponible
            if result.get('date_parsed'):
                try:
                    if isinstance(result['date_parsed'], str):
                        result_date = datetime.fromisoformat(result['date_parsed'])
                    else:
                        result_date = result['date_parsed']
                    
                    if start_date <= result_date <= end_date:
                        filtered.append(result)
                except:
                    # Inclure avec score rÃ©duit si erreur
                    result['relevance_score'] = result.get('relevance_score', 0.5) * 0.7
                    filtered.append(result)
            else:
                # Pas de date, inclure selon le type
                if time_constraint not in ['this_week', 'today']:
                    result['relevance_score'] = result.get('relevance_score', 0.5) * 0.5
                    filtered.append(result)
        
        return filtered
    
    def _generate_final_response(
        self,
        user_query: str,
        search_results: List[Dict],
        search_query: str,
        current_date: Optional[datetime],
        time_constraint: Optional[str]
    ) -> str:
        """
        GÃ©nÃ¨re la rÃ©ponse finale en utilisant les rÃ©sultats de recherche
        """
        if not search_results:
            return f"Je n'ai pas trouvÃ© d'informations rÃ©centes pour votre recherche \"{search_query}\". Essayez de reformuler votre question ou de prÃ©ciser ce que vous cherchez."
        
        # Formater le contexte
        context = self._format_search_context(search_results)
        
        # Informations temporelles
        date_info = ""
        if current_date:
            date_info = f"""
ğŸ“… DATE ACTUELLE: {current_date.strftime('%d/%m/%Y')}
ğŸ“… SEMAINE: {current_date.isocalendar()[1]} de {current_date.year}
â° PÃ‰RIODE DEMANDÃ‰E: {time_constraint or 'Non spÃ©cifiÃ©e'}
ğŸ” REQUÃŠTE DE RECHERCHE UTILISÃ‰E: "{search_query}"
"""
        
        # Prompt pour la rÃ©ponse finale
        system_prompt = f"""Tu es un assistant IA expert qui rÃ©pond aux questions en utilisant EXCLUSIVEMENT les informations des rÃ©sultats de recherche fournis.
{date_info}

ğŸ”´ RÃˆGLES ABSOLUES:
1. Tu DOIS baser ta rÃ©ponse UNIQUEMENT sur le contexte ci-dessous
2. Tu DOIS citer chaque information avec [Source: Titre de l'article]
3. Si une information n'est pas dans le contexte, dis "Cette information n'est pas disponible dans les sources trouvÃ©es"
4. Structure ta rÃ©ponse de maniÃ¨re claire et organisÃ©e
5. Termine TOUJOURS par une section "ğŸ“š Sources consultÃ©es:" avec les titres et URLs

ğŸ“° RÃ‰SULTATS DE RECHERCHE (UTILISE UNIQUEMENT CES INFORMATIONS):
{context}

âš ï¸ NE PAS inventer ou utiliser des connaissances non prÃ©sentes dans le contexte ci-dessus."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ]
        
        try:
            # Utiliser le modÃ¨le sÃ©lectionnÃ© dans le cache
            selected_model = cache.get('selected_llm_model', 'vllm')
            logger.info(f"ğŸ” GÃ©nÃ©ration rÃ©ponse recherche avec: {selected_model}")
            
            if selected_model == 'vllm' and self.vllm_service.is_available():
                # Utiliser vLLM
                prompt = f"{system_prompt}\n\nQuestion: {user_query}"
                response = self.vllm_service.generate_response(prompt=prompt)
                if response['success']:
                    return response['response']
                else:
                    logger.error(f"Erreur vLLM: {response['error']}")
                    # Fallback vers OpenRouter
                    selected_model = 'openrouter'
            
            # Si OpenRouter ou si vLLM a Ã©chouÃ©
            if selected_model == 'openrouter':
                response = httpx.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json={
                        "model": self.model,
                        "messages": messages,
                        "temperature": 0.3,
                        "max_tokens": 2000
                    },
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result['choices'][0]['message']['content']
                elif response.status_code == 429:
                    logger.error("âš ï¸ Limite de taux OpenRouter atteinte (429)")
                    return "âš ï¸ Limite de requÃªtes OpenRouter atteinte. Veuillez patienter quelques minutes ou utiliser vLLM local."
                else:
                    logger.error(f"Erreur gÃ©nÃ©ration rÃ©ponse: {response.status_code}")
                    return f"Erreur OpenRouter ({response.status_code}). Essayez vLLM local ou rÃ©essayez plus tard."
                
        except Exception as e:
            logger.error(f"Erreur rÃ©ponse finale: {e}")
            return "Une erreur s'est produite lors de la gÃ©nÃ©ration de la rÃ©ponse."
    
    def _format_search_context(self, search_results: List[Dict]) -> str:
        """Formate les rÃ©sultats pour le contexte"""
        formatted = []
        
        for i, result in enumerate(search_results[:10], 1):  # Max 10 rÃ©sultats
            title = result.get('title', 'Sans titre')
            url = result.get('url', '#')
            content = result.get('content', 'Contenu non disponible')
            date = result.get('date', 'Date non spÃ©cifiÃ©e')
            source = result.get('source', 'Source inconnue')
            
            formatted_result = f"""
=== RÃ‰SULTAT {i} ===
ğŸ“° TITRE: {title}
ğŸŒ SOURCE: {source}
ğŸ“… DATE DE PUBLICATION: {date}
ğŸ”— URL: {url}
ğŸ“ CONTENU:
{content}
{'='*60}"""
            formatted.append(formatted_result)
        
        return "\n".join(formatted)
"""
Service Hugging Face pour utiliser Qwen2.5-32B-Instruct
"""
import logging
from typing import List, Dict, Optional, Any
from datetime import datetime
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import os

logger = logging.getLogger(__name__)


class HuggingFaceService:
    """Service pour utiliser le modÃ¨le Qwen via Hugging Face"""
    
    def __init__(self):
        """Initialise le service avec le modÃ¨le Qwen"""
        self.model_name = "Qwen/Qwen2.5-32B-Instruct"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ğŸ¤– Initialisation du modÃ¨le {self.model_name}")
        logger.info(f"ğŸ–¥ï¸ Device: {self.device}")
        
        try:
            # Pour Ã©conomiser la mÃ©moire, on utilise le pipeline qui gÃ¨re automatiquement
            # le chargement et l'optimisation
            self.pipe = pipeline(
                "text-generation",
                model=self.model_name,
                device=self.device,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                # Limiter l'utilisation mÃ©moire
                model_kwargs={
                    "load_in_8bit": True if self.device == "cuda" else False,
                    "low_cpu_mem_usage": True
                }
            )
            logger.info("âœ… ModÃ¨le chargÃ© avec succÃ¨s")
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement du modÃ¨le: {e}")
            # Fallback vers un modÃ¨le plus petit si nÃ©cessaire
            logger.info("ğŸ”„ Tentative avec un modÃ¨le plus petit...")
            self.model_name = "Qwen/Qwen2.5-7B-Instruct"
            try:
                self.pipe = pipeline(
                    "text-generation",
                    model=self.model_name,
                    device=self.device,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                )
                logger.info(f"âœ… ModÃ¨le de fallback {self.model_name} chargÃ©")
            except Exception as e2:
                logger.error(f"âŒ Impossible de charger le modÃ¨le: {e2}")
                raise
    
    def generate_response(
        self, 
        query: str, 
        search_results: Optional[List[Dict]] = None,
        current_date: Optional[datetime] = None,
        time_constraint: Optional[str] = None,
        conversation_history: Optional[List[Dict]] = None
    ) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse en utilisant le modÃ¨le Qwen
        """
        logger.info(f"\nğŸ¤– HUGGING FACE - GÃ©nÃ©ration de rÃ©ponse")
        logger.info(f"ğŸ“Š ModÃ¨le: {self.model_name}")
        
        try:
            messages = []
            
            # Si des rÃ©sultats de recherche sont fournis
            if search_results:
                system_content = self._create_system_prompt_with_context(
                    search_results, 
                    current_date, 
                    time_constraint
                )
                messages.append({
                    "role": "system",
                    "content": system_content
                })
                
                # Ajouter l'historique si prÃ©sent
                if conversation_history:
                    for msg in conversation_history[-3:]:  # Derniers 3 messages pour Ã©conomiser les tokens
                        if msg['role'] in ['user', 'assistant']:
                            messages.append({
                                "role": msg['role'],
                                "content": msg['content']
                            })
            else:
                # Sans recherche web
                messages.append({
                    "role": "system",
                    "content": "Tu es un assistant IA utile et amical. RÃ©ponds de maniÃ¨re claire et concise en franÃ§ais."
                })
                
                if conversation_history:
                    for msg in conversation_history[-3:]:
                        if msg['role'] in ['user', 'assistant']:
                            messages.append({
                                "role": msg['role'],
                                "content": msg['content']
                            })
            
            # Ajouter la question actuelle
            messages.append({
                "role": "user",
                "content": query
            })
            
            logger.info(f"ğŸ“ Nombre de messages: {len(messages)}")
            
            # GÃ©nÃ©rer la rÃ©ponse
            outputs = self.pipe(
                messages,
                max_new_tokens=1500,
                temperature=0.3,
                do_sample=True,
                top_p=0.9,
                return_full_text=False
            )
            
            # Extraire la rÃ©ponse
            response = outputs[0]['generated_text']
            
            # Valider l'utilisation des sources si recherche
            if search_results and not self._validate_source_usage(response, search_results):
                logger.warning("âš ï¸ RÃ©ponse sans citations suffisantes")
                response = self._add_source_reminder(response, search_results)
            
            return response
            
        except Exception as e:
            logger.error(f"Erreur Hugging Face: {str(e)}")
            return f"DÃ©solÃ©, une erreur s'est produite lors de la gÃ©nÃ©ration de la rÃ©ponse: {str(e)}"
    
    def _create_system_prompt_with_context(
        self, 
        search_results: List[Dict], 
        current_date: Optional[datetime],
        time_constraint: Optional[str]
    ) -> str:
        """CrÃ©e un prompt systÃ¨me avec le contexte de recherche"""
        
        # Informations temporelles
        date_info = ""
        if current_date:
            date_info = f"""
ğŸ“… DATE ACTUELLE: {current_date.strftime('%d/%m/%Y')}
ğŸ“… SEMAINE ACTUELLE: Semaine {current_date.isocalendar()[1]} de {current_date.year}
â° PÃ‰RIODE DEMANDÃ‰E: {time_constraint or 'Non spÃ©cifiÃ©e'}
"""
        
        # Formater le contexte
        context = self._format_search_results(search_results)
        
        # Prompt systÃ¨me
        system_prompt = f"""Tu es un assistant IA expert en actualitÃ©s technologiques. Tu DOIS utiliser EXCLUSIVEMENT les informations fournies dans le contexte de recherche ci-dessous.
{date_info}

ğŸ”´ RÃˆGLES OBLIGATOIRES:
1. Base ta rÃ©ponse UNIQUEMENT sur le contexte fourni
2. Cite chaque information avec [Source: Titre]
3. Si une info n'est pas dans le contexte, dis-le clairement
4. Termine par "ğŸ“š Sources consultÃ©es:" avec les URLs

ğŸ“° CONTEXTE DE RECHERCHE:
{context}

âš ï¸ RAPPEL: Utilise UNIQUEMENT les informations ci-dessus."""
        
        return system_prompt
    
    def _format_search_results(self, search_results: List[Dict]) -> str:
        """Formate les rÃ©sultats de recherche"""
        if not search_results:
            return "Aucun rÃ©sultat disponible."
        
        formatted = []
        for i, result in enumerate(search_results[:8], 1):  # Max 8 pour limiter les tokens
            title = result.get('title', 'Sans titre')
            url = result.get('url', '#')
            content = result.get('content', 'Non disponible')[:300]  # Limiter la longueur
            date = result.get('date', 'Date non spÃ©cifiÃ©e')
            
            formatted.append(f"""
=== RÃ‰SULTAT {i} ===
ğŸ“° {title}
ğŸ“… {date}
ğŸ”— {url}
ğŸ“ {content}...
""")
        
        return "\n".join(formatted)
    
    def _validate_source_usage(self, response: str, search_results: List[Dict]) -> bool:
        """VÃ©rifie que la rÃ©ponse utilise les sources"""
        response_lower = response.lower()
        
        # Patterns de citation
        citation_patterns = ["[source:", "source:", "selon", "d'aprÃ¨s", "sources consultÃ©es"]
        
        # Compter les citations
        citation_count = sum(1 for pattern in citation_patterns if pattern in response_lower)
        
        # VÃ©rifier si au moins 30% des sources sont citÃ©es
        min_citations = max(1, len(search_results) // 3)
        
        return citation_count >= min_citations
    
    def _add_source_reminder(self, response: str, search_results: List[Dict]) -> str:
        """Ajoute les sources si non citÃ©es"""
        
        if "sources consultÃ©es" in response.lower():
            return response
        
        # Ajouter une section sources
        sources_section = "\n\nğŸ“š **Sources consultÃ©es:**\n"
        for i, result in enumerate(search_results[:5], 1):
            title = result.get('title', 'Sans titre')
            url = result.get('url', '#')
            sources_section += f"{i}. {title}\n   ğŸ”— {url}\n"
        
        return response + sources_section
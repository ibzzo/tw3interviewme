# Test Technique - IngÃ©nieur IA GÃ©nÃ©rative

Ce dÃ©pÃ´t contient l'implÃ©mentation complÃ¨te du test technique avec deux projets :

1. **chatbot-ia-generative/** - Partie 1 & 2 : Chatbot avec accÃ¨s Internet
2. **vlm_project/** - Partie 3 : Vision Language Model (Qwen2.5-VL)

## ğŸ“ Structure du dÃ©pÃ´t

```
entretient/
â”œâ”€â”€ chatbot-ia-generative/    # Partie 1 & 2
â”‚   â”œâ”€â”€ backend/             # API Django
â”‚   â”œâ”€â”€ frontend/            # Interface React
â”‚   â””â”€â”€ README.md           # Documentation dÃ©taillÃ©e
â”œâ”€â”€ vlm_project/             # Partie 3
â”‚   â”œâ”€â”€ vlm_demo.py         # Script VLM
â”‚   â”œâ”€â”€ start_vllm.sh       # Script de dÃ©marrage optimisÃ©
â”‚   â””â”€â”€ README.md           # Documentation + Question thÃ©orique
â””â”€â”€ instruct.pdf            # Instructions du test

```

## ğŸš€ AccÃ¨s rapide

- **[Partie 3 : Vision Language Model â†’](../vlm_project/README.md)**
  - ImplÃ©mentation Qwen2.5-VL locale
  - Script de dÃ©monstration VLM
  - RÃ©ponse Ã  la question thÃ©orique sur l'architecture VLM-RAG

---

# Partie 1 & 2 : Chatbot IA GÃ©nÃ©rative avec AccÃ¨s Internet

## ğŸ“‹ Table des matiÃ¨res
1. [Vue d'ensemble](#vue-densemble)
2. [Architecture](#architecture)
3. [Installation](#installation)
4. [Utilisation](#utilisation)
5. [FonctionnalitÃ©s](#fonctionnalitÃ©s)
6. [Questions thÃ©oriques](#questions-thÃ©oriques)
7. [Plan de dÃ©ploiement Azure](#plan-de-dÃ©ploiement-azure)
8. [Challenge VLM (Bonus)](#challenge-vlm-bonus)

## ğŸ¯ Vue d'ensemble

Ce projet implÃ©mente un chatbot IA gÃ©nÃ©rative avec capacitÃ© de recherche web, utilisant OpenRouter avec le modÃ¨le **`qwen/qwen-2.5-32b-instruct:free`** (comme spÃ©cifiÃ© dans les instructions du test technique) et SerpAPI/DuckDuckGo pour les recherches. L'architecture suit les meilleures pratiques avec une sÃ©paration claire entre backend (Django) et frontend (React).

### Points forts de l'implÃ©mentation :
- âœ… Architecture modulaire et scalable
- âœ… Gestion robuste des erreurs
- âœ… Rate limiting intÃ©grÃ©
- âœ… Cache de recherche pour optimiser les performances
- âœ… Interface utilisateur moderne et rÃ©active
- âœ… Support des sources avec citations

## ğŸ— Architecture

```
chatbot-ia-generative/
â”œâ”€â”€ backend/                    # Django Backend
â”‚   â”œâ”€â”€ chatbot_backend/       # Configuration principale
â”‚   â”œâ”€â”€ chat/                  # Application principale
â”‚   â”‚   â”œâ”€â”€ models.py          # ModÃ¨les de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ views.py           # API endpoints
â”‚   â”‚   â”œâ”€â”€ serializers.py     # SÃ©rialisation DRF
â”‚   â”‚   â””â”€â”€ services/          # Services mÃ©tier
â”‚   â”‚       â”œâ”€â”€ openrouter_service.py
â”‚   â”‚       â””â”€â”€ search_service.py
â”œâ”€â”€ frontend/                   # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # Composants React
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageList.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageInput.tsx
â”‚   â”‚   â”‚   â””â”€â”€ SourcesList.tsx
â”‚   â”‚   â””â”€â”€ App.tsx           # Composant principal
â”œâ”€â”€ .env                       # Variables d'environnement
â””â”€â”€ requirements.txt           # DÃ©pendances Python
```

## ğŸš€ Installation

### PrÃ©requis
- Python 3.8+
- Node.js 14+
- Redis (optionnel, pour le cache)

### Backend

```bash
# Cloner le repository
git clone [URL_DU_REPO]
cd chatbot-ia-generative

# CrÃ©er environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt

# Configurer les variables d'environnement
cp .env.example .env
# Ã‰diter .env et ajouter votre OPENROUTER_API_KEY

# Migrations Django
python manage.py makemigrations
python manage.py migrate

# Lancer le serveur
python manage.py runserver
```

### Frontend

```bash
# Dans un nouveau terminal
cd frontend
npm install
npm start
```

## ğŸ’¬ Utilisation

1. Ouvrir http://localhost:3000 dans votre navigateur
2. Taper votre message dans la zone de texte
3. Le chatbot analysera si une recherche web est nÃ©cessaire
4. Les sources seront affichÃ©es Ã  droite si des recherches sont effectuÃ©es

### Exemple de requÃªte test :
```
"Quels sont les derniers dÃ©veloppements en IA gÃ©nÃ©rative annoncÃ©s cette semaine ? Donne-moi 3 exemples concrets avec leurs sources."
```

## âœ¨ FonctionnalitÃ©s

### 1. Recherche Web Intelligente
- DÃ©tection automatique des requÃªtes nÃ©cessitant une recherche
- Cache des rÃ©sultats pour Ã©viter les recherches redondantes
- Extraction et formatage des informations pertinentes

### 2. Service LLM
- Utilisation d'OpenRouter avec le modÃ¨le Qwen optimisÃ©
- GÃ©nÃ©ration de rÃ©ponses contextuelles basÃ©es sur les recherches web
- Support de l'historique de conversation

### 3. Gestion des Erreurs
- Retry automatique en cas d'Ã©chec API
- Messages d'erreur clairs pour l'utilisateur
- Logging structurÃ© pour le debugging

### 4. Rate Limiting
- Protection contre les abus (10 requÃªtes/minute par dÃ©faut)
- Configurable via variables d'environnement

### 5. Performance
- RequÃªtes asynchrones pour une meilleure rÃ©activitÃ©
- Cache Redis pour les recherches frÃ©quentes
- Optimisation des appels API

## ğŸ“š Questions thÃ©oriques

### 1. Architecture et DÃ©ploiement en Production

#### Plan de dÃ©ploiement Azure (TRÃˆS IMPORTANT)

##### Ã‰tape 1 : PrÃ©requis Azure
- **Compte Azure** avec souscription active
- **Azure CLI** installÃ© localement
- **Permissions requises** :
  - Contributor sur la souscription
  - User Access Administrator pour les rÃ´les IAM
  - Key Vault Secrets Officer pour les secrets

##### Ã‰tape 2 : Services Azure Ã  provisionner

1. **Resource Group**
   ```bash
   az group create --name rg-chatbot-prod --location francecentral
   ```

2. **Azure Container Registry (ACR)**
   ```bash
   az acr create --resource-group rg-chatbot-prod \
     --name chatbotacr --sku Basic
   ```

3. **Azure App Service Plan + Web Apps**
   ```bash
   # App Service Plan
   az appservice plan create --name asp-chatbot \
     --resource-group rg-chatbot-prod \
     --sku B2 --is-linux
   
   # Backend Web App
   az webapp create --resource-group rg-chatbot-prod \
     --plan asp-chatbot --name chatbot-backend-api \
     --runtime "PYTHON:3.9"
   
   # Frontend Static Web App
   az staticwebapp create --name chatbot-frontend \
     --resource-group rg-chatbot-prod
   ```

4. **Azure Database for PostgreSQL**
   ```bash
   az postgres flexible-server create \
     --resource-group rg-chatbot-prod \
     --name chatbot-db-server \
     --admin-user adminuser \
     --admin-password <secure-password> \
     --sku-name Standard_B2s \
     --storage-size 32
   ```

5. **Azure Cache for Redis**
   ```bash
   az redis create --location francecentral \
     --name chatbot-redis \
     --resource-group rg-chatbot-prod \
     --sku Basic --vm-size c0
   ```

6. **Azure Key Vault**
   ```bash
   az keyvault create --name chatbot-keyvault \
     --resource-group rg-chatbot-prod \
     --location francecentral
   ```

7. **Application Insights**
   ```bash
   az monitor app-insights component create \
     --app chatbot-insights \
     --location francecentral \
     --resource-group rg-chatbot-prod
   ```

##### Ã‰tape 3 : Architecture Scalable

```mermaid
graph TB
    A[Azure Front Door] --> B[App Service - Frontend]
    A --> C[App Service - Backend API]
    C --> D[PostgreSQL Flexible Server]
    C --> E[Redis Cache]
    C --> F[Key Vault]
    C --> G[Application Insights]
    H[Azure Container Registry] --> C
```

##### Ã‰tape 4 : Estimation des coÃ»ts mensuels

| Service | Configuration | CoÃ»t estimÃ©/mois |
|---------|--------------|------------------|
| App Service Plan | B2 (2 cores, 3.5GB RAM) | ~80â‚¬ |
| PostgreSQL | B2s (2 vCores, 4GB RAM) | ~60â‚¬ |
| Redis Cache | C0 (250MB) | ~15â‚¬ |
| Key Vault | Standard | ~5â‚¬ |
| Application Insights | 5GB/mois | ~12â‚¬ |
| Static Web App | Free tier | 0â‚¬ |
| **TOTAL** | | **~172â‚¬/mois** |

##### Ã‰tape 5 : SÃ©curitÃ©

1. **Secrets Management**
   - Toutes les clÃ©s API dans Key Vault
   - Managed Identity pour l'accÃ¨s aux services
   - Rotation automatique des secrets

2. **Network Security**
   - VNet integration pour les services
   - Private endpoints pour PostgreSQL et Redis
   - WAF sur Front Door

3. **Authentication**
   - Azure AD B2C pour l'authentification utilisateur
   - API Management pour la gestion des API keys

##### Ã‰tape 6 : CI/CD Pipeline

```yaml
# azure-pipelines.yml
trigger:
  - main

pool:
  vmImage: 'ubuntu-latest'

stages:
- stage: Build
  jobs:
  - job: BuildBackend
    steps:
    - script: |
        docker build -t chatbot-backend ./backend
        docker tag chatbot-backend $(ACR_NAME).azurecr.io/chatbot-backend:$(Build.BuildId)
    - task: Docker@2
      inputs:
        command: push
        repository: chatbot-backend
        containerRegistry: $(ACR_CONNECTION)

- stage: Deploy
  jobs:
  - deployment: DeployProd
    environment: 'production'
    strategy:
      runOnce:
        deploy:
          steps:
          - task: AzureWebApp@1
            inputs:
              azureSubscription: $(AZURE_SUBSCRIPTION)
              appName: 'chatbot-backend-api'
              images: '$(ACR_NAME).azurecr.io/chatbot-backend:$(Build.BuildId)'
```

#### StratÃ©gie de mise en production

1. **Monitoring et Logs**
   - Application Insights pour les mÃ©triques
   - Log Analytics Workspace pour centraliser les logs
   - Alertes configurÃ©es sur les mÃ©triques clÃ©s

2. **Gestion des erreurs**
   - Circuit breaker pattern pour les appels externes
   - Dead letter queue pour les messages non traitÃ©s
   - Retry policies avec backoff exponentiel

3. **Backup Strategy**
   - Backup automatique PostgreSQL (7 jours de rÃ©tention)
   - Geo-replication pour la haute disponibilitÃ©
   - Point-in-time restore capability

## ğŸ”§ Challenge VLM (Bonus) - Partie 3

### Script VLM Local

Le fichier `vlm_demo.py` implÃ©mente une dÃ©monstration de Vision Language Model adaptÃ©e aux contraintes techniques :

**âš ï¸ Limitations techniques rencontrÃ©es :**
- **Pas de GPU avec 8GB VRAM** (requis pour les VLM <3B paramÃ¨tres)
- **Python 3.13 incompatible avec vLLM**
- Solution alternative fournie avec modÃ¨le lÃ©ger CPU

**Utilisation du script :**
```bash
python vlm_demo.py image.jpg
```

**Configuration idÃ©ale pour VLM complet :**
1. Python 3.12 ou infÃ©rieur (vLLM ne supporte pas 3.13)
2. GPU avec minimum 8GB VRAM
3. Installation : `pip install vllm`
4. DÃ©marrage : `vllm serve "Qwen/Qwen2.5-VL-3B-Instruct"`

**Script alternatif fourni :**
- Utilise BLIP (modÃ¨le lÃ©ger compatible CPU)
- Extraction de texte et description d'images
- Fallback OCR si modÃ¨le non disponible

### Architecture pour systÃ¨me de vectorisation avec VLMs

Pour un pipeline RAG intÃ©grant des VLMs, voici mon approche :

#### 1. Pipeline de traitement multimodal

```python
class MultimodalVectorizer:
    def __init__(self):
        self.text_encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.image_encoder = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')
        self.vlm = AutoModelForVision2Seq.from_pretrained('Qwen/Qwen2-VL-2B')
        
    def process_document(self, doc_path):
        # 1. Extraction des Ã©lÃ©ments
        text_chunks = extract_text(doc_path)
        images = extract_images(doc_path)
        
        # 2. Enrichissement via VLM
        enriched_data = []
        for img in images:
            description = self.vlm.generate(img, "Describe this image in detail")
            enriched_data.append({
                'type': 'image',
                'original': img,
                'description': description,
                'embedding': self.image_encoder.encode(img)
            })
        
        # 3. Vectorisation hybride
        for chunk in text_chunks:
            enriched_data.append({
                'type': 'text',
                'content': chunk,
                'embedding': self.text_encoder.encode(chunk)
            })
        
        return enriched_data
```

#### 2. StratÃ©gie de stockage

```yaml
Vector Store Structure:
  - Collections:
    - text_embeddings (dimension: 384)
    - image_embeddings (dimension: 512)
    - metadata:
      - source_document
      - chunk_type
      - vlm_description
      - position_in_doc
```

#### 3. Recherche hybride

```python
def hybrid_search(query, include_images=True):
    # Recherche textuelle
    text_results = vector_store.search_text(query, k=5)
    
    if include_images:
        # GÃ©nÃ©ration d'une image query via texte
        image_query_embedding = clip_text_encoder(query)
        image_results = vector_store.search_images(image_query_embedding, k=3)
        
        # Fusion des rÃ©sultats avec re-ranking
        combined = rerank_results(text_results + image_results, query)
    
    return combined
```

#### 4. Avantages de cette approche

1. **ComprÃ©hension enrichie** : Les VLMs ajoutent du contexte aux images
2. **Recherche cross-modale** : PossibilitÃ© de chercher des images avec du texte
3. **Meilleure pertinence** : Les descriptions VLM amÃ©liorent le matching
4. **FlexibilitÃ©** : Support de diffÃ©rents types de documents

## ğŸ“ Conclusion

Ce projet dÃ©montre une implÃ©mentation complÃ¨te d'un chatbot IA avec recherche web, suivant les meilleures pratiques en termes d'architecture, sÃ©curitÃ© et scalabilitÃ©. L'approche modulaire permet une Ã©volution facile et l'ajout de nouvelles fonctionnalitÃ©s.
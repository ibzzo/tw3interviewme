# Chatbot IA avec Recherche Web et LLM Local/Cloud

## ğŸ“‹ Description du projet

Chatbot intelligent dÃ©veloppÃ© pour le test technique d'IngÃ©nieur IA GÃ©nÃ©rative. Le systÃ¨me combine :
- **Recherche web intelligente** avec intÃ©gration de multiples sources
- **Double mode LLM** : Local (vLLM) ou Cloud (OpenRouter)
- **Interface moderne** avec panneau de sources et sÃ©lecteur de modÃ¨le

## âœ¨ FonctionnalitÃ©s implÃ©mentÃ©es

### Partie 1 : Chatbot avec accÃ¨s Internet
- âœ… Recherche web multi-sources (SerpAPI, fallbacks)
- âœ… IntÃ©gration LLM (OpenRouter avec Qwen-2.5-32b gratuit)
- âœ… Interface React moderne et responsive
- âœ… Gestion des erreurs et limites de taux
- âœ… **Bonus** : LLM local avec vLLM (Phi-3)

### FonctionnalitÃ©s avancÃ©es
- ğŸ¯ **SÃ©lecteur de modÃ¨le** : Bascule facile entre local/cloud
- ğŸ“š **Panneau de sources** : Affichage Ã©lÃ©gant des rÃ©fÃ©rences
- ğŸ” **Recherche intelligente** : Optimisation automatique des requÃªtes
- âš¡ **Cache intelligent** : Performance optimisÃ©e

## ğŸ› ï¸ Installation

### PrÃ©requis
- Python 3.13 (Django) + Python 3.12 (vLLM)
- Node.js 14+
- macOS/Linux recommandÃ©

### 1. Backend Django (Python 3.13)
```bash
cd chatbot-ia-generative
python3.13 -m venv venv
source venv/bin/activate

# Installer : Django, DRF, requests, python-dotenv, django-cors-headers
pip install -r requirements.txt

# Configuration
cp .env.example .env  # Ajouter votre clÃ© OpenRouter

# Base de donnÃ©es
python manage.py migrate
```

### 2. vLLM (optionnel pour mode local)

#### Installation (Python 3.12 requis)
```bash
python3.12 -m venv venv_vllm
source venv_vllm/bin/activate

# Installer uniquement vLLM (inclut torch, transformers automatiquement)
pip install vllm
```

#### Configuration et lancement
```bash
# Activer l'environnement
source venv_vllm/bin/activate

# Configurer pour CPU
export VLLM_CPU_KVCACHE_SPACE=8

# Lancer le serveur vLLM
vllm serve "microsoft/Phi-3-mini-4k-instruct" \
    --host 0.0.0.0 \
    --port 8080 \
    --device cpu
```

#### ModÃ¨les recommandÃ©s
- **microsoft/Phi-3-mini-4k-instruct** (3.8B) - RecommandÃ©, bon Ã©quilibre
- **microsoft/phi-2** (2.7B) - Plus rapide sur CPU
- **google/gemma-2b** (2B) - Ultra lÃ©ger

#### RÃ©solution des problÃ¨mes
- **Port dÃ©jÃ  utilisÃ©** : Changer avec `--port 8081`
- **MÃ©moire insuffisante** : RÃ©duire avec `--max-model-len 2048`
- **Module non trouvÃ©** : VÃ©rifier `which python` pointe vers venv_vllm

### 3. Frontend React
```bash
cd frontend
npm install
```

## ğŸš€ Lancement

### Mode 1 : Cloud uniquement (2 terminaux)
```bash
# Terminal 1 - Backend Django
source venv/bin/activate        # âš ï¸ venv (pas venv_vllm)
python manage.py runserver

# Terminal 2 - Frontend React
cd frontend && npm start
```

### Mode 2 : Avec vLLM local (3 terminaux)
```bash
# Terminal 1 - vLLM
source venv_vllm/bin/activate   # âš ï¸ venv_vllm (pas venv)
export VLLM_CPU_KVCACHE_SPACE=8
vllm serve "microsoft/Phi-3-mini-4k-instruct" --host 0.0.0.0 --port 8080

# Terminal 2 - Backend Django
source venv/bin/activate        # âš ï¸ venv (pas venv_vllm)
python manage.py runserver

# Terminal 3 - Frontend React
cd frontend && npm start
```

AccÃ¨s : http://localhost:3000

## ğŸ“ ScÃ©nario de test

**Question type** : "Quels sont les derniers dÃ©veloppements en IA gÃ©nÃ©rative annoncÃ©s cette semaine ? Donne-moi 3 exemples concrets avec leurs sources."

**RÃ©sultat attendu** :
- DÃ©tection automatique du besoin de recherche
- Recherche web optimisÃ©e avec contrainte temporelle
- SynthÃ¨se avec 3 exemples concrets
- Sources affichÃ©es dans le panneau latÃ©ral

---

# Partie 2 : Questions thÃ©oriques

## 3.1 Architecture et DÃ©ploiement Azure

### Plan de dÃ©ploiement Ã©tape par Ã©tape

#### 1. PrÃ©requis Azure
- **Souscription Azure** active
- **Permissions nÃ©cessaires** :
  - Contributeur sur la souscription
  - Administrateur Azure AD (pour les identitÃ©s managÃ©es)
  - Contributeur Key Vault
- **Outils** : Azure CLI, Docker, kubectl

#### 2. Architecture cible sur Azure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Azure Front Door  â”‚ â† CDN + WAF + Load Balancer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  App Service Plan   â”‚â”€â”€â”€â”€â–¶â”‚   Azure Cache   â”‚
â”‚  - Frontend (React) â”‚     â”‚   for Redis     â”‚
â”‚  - Backend (Django) â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
           â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure Container     â”‚     â”‚   Key Vault     â”‚
â”‚ Instance (vLLM)     â”‚     â”‚  (API Keys)     â”‚
â”‚ avec GPU (optionnel)â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Azure Database     â”‚
â”‚  for PostgreSQL     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. Services Azure requis

| Service | Utilisation | SKU recommandÃ© |
|---------|-------------|----------------|
| App Service | Frontend + Backend | P1v3 (Production) |
| PostgreSQL | Base de donnÃ©es | Basic 2vCore |
| Redis Cache | Cache + Sessions | Basic C1 |
| Key Vault | Secrets | Standard |
| Container Instance | vLLM (optionnel) | GPU NC6 |
| Front Door | CDN + SÃ©curitÃ© | Standard |
| Application Insights | Monitoring | Pay-as-you-go |

#### 4. Estimation des coÃ»ts mensuels

| Service | Configuration | CoÃ»t estimÃ© |
|---------|--------------|-------------|
| App Service (P1v3) | 2 instances | 180â‚¬ |
| PostgreSQL | Basic 2vCore | 50â‚¬ |
| Redis | Basic C1 | 40â‚¬ |
| Front Door | Standard | 35â‚¬ |
| Key Vault | Standard | 5â‚¬ |
| Application Insights | 5GB/mois | 25â‚¬ |
| **Sans GPU** | | **~335â‚¬/mois** |
| Container Instance GPU | NC6 (optionnel) | +500â‚¬ |
| **Avec GPU** | | **~835â‚¬/mois** |

#### 5. DÃ©ploiement pas Ã  pas

```bash
# 1. Connexion et crÃ©ation du groupe
az login
az group create --name rg-chatbot-prod --location westeurope

# 2. Key Vault pour les secrets
az keyvault create \
  --name kv-chatbot-unique \
  --resource-group rg-chatbot-prod \
  --location westeurope

# Ajouter les secrets
az keyvault secret set \
  --vault-name kv-chatbot-unique \
  --name "openrouter-api-key" \
  --value "YOUR_KEY"

# 3. PostgreSQL
az postgres flexible-server create \
  --resource-group rg-chatbot-prod \
  --name chatbot-db-prod \
  --location westeurope \
  --tier Burstable \
  --sku-name B_Standard_B1ms \
  --storage-size 32 \
  --version 15

# 4. Redis Cache
az redis create \
  --location westeurope \
  --name chatbot-redis \
  --resource-group rg-chatbot-prod \
  --sku Basic \
  --vm-size c1

# 5. App Service Plan
az appservice plan create \
  --name asp-chatbot \
  --resource-group rg-chatbot-prod \
  --location westeurope \
  --sku P1v3 \
  --is-linux

# 6. Web Apps
az webapp create \
  --resource-group rg-chatbot-prod \
  --plan asp-chatbot \
  --name chatbot-backend-prod \
  --runtime "PYTHON:3.11"

az webapp create \
  --resource-group rg-chatbot-prod \
  --plan asp-chatbot \
  --name chatbot-frontend-prod \
  --runtime "NODE:18-lts"

# 7. Front Door
az network front-door create \
  --resource-group rg-chatbot-prod \
  --name fd-chatbot \
  --backend-address chatbot-frontend-prod.azurewebsites.net
```

#### 6. Points d'attention critiques

**AccÃ¨s et permissions** :
- âœ… Utiliser les Managed Identities pour l'accÃ¨s aux services
- âœ… RBAC pour limiter les accÃ¨s par rÃ´le
- âœ… Network Security Groups pour isoler les composants

**SÃ©curitÃ© des secrets** :
- âœ… Toutes les clÃ©s API dans Key Vault
- âœ… Rotation automatique des secrets
- âœ… Audit logs activÃ©s

**PrÃ©requis techniques** :
- Domaine custom pour Front Door
- Certificats SSL (gÃ©rÃ© par Azure)
- Firewall rules pour PostgreSQL

#### 7. CI/CD Pipeline

```yaml
# azure-pipelines.yml
trigger:
  branches:
    include: ['main']

stages:
- stage: Build
  jobs:
  - job: BuildBackend
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - task: Docker@2
      inputs:
        command: 'buildAndPush'
        repository: 'chatbot-backend'
        dockerfile: 'Dockerfile'
        
  - job: BuildFrontend
    steps:
    - task: NodeTool@0
      inputs:
        versionSpec: '18.x'
    - script: |
        cd frontend
        npm install
        npm run build
        
- stage: Deploy
  jobs:
  - deployment: DeployProd
    environment: 'production'
    strategy:
      runOnce:
        deploy:
          steps:
          - task: AzureWebApp@1
            inputs:
              appType: 'webAppLinux'
              appName: 'chatbot-backend-prod'
```

#### 8. Monitoring et logs

- **Application Insights** : IntÃ©grÃ© dans le code Django/React
- **Log Analytics** : Centralisation des logs
- **Alertes** : CPU > 80%, Erreurs > 10/min
- **Dashboards** : MÃ©triques temps rÃ©el

### StratÃ©gie de backup

1. **PostgreSQL** : Backup automatique quotidien (rÃ©tention 7 jours)
2. **Code** : Git + Azure Repos
3. **Configurations** : Infrastructure as Code (Terraform/ARM)

---

## Architecture simplifiÃ©e

```
chatbot-ia-generative/
â”œâ”€â”€ chat/                       # Backend Django
â”‚   â”œâ”€â”€ services/              # Services essentiels uniquement
â”‚   â”‚   â”œâ”€â”€ vllm_service.py   # LLM local (Phi-3)
â”‚   â”‚   â”œâ”€â”€ openrouter_optimized.py   # LLM cloud (Qwen)
â”‚   â”‚   â”œâ”€â”€ intelligent_search.py     # Recherche web intelligente
â”‚   â”‚   â”œâ”€â”€ serpapi_service.py        # API de recherche principale
â”‚   â”‚   â””â”€â”€ multi_search.py           # Fallback de recherche
â”‚   â”œâ”€â”€ views.py              # Endpoints API (/chat, /set-model)
â”‚   â””â”€â”€ models.py             # Base de donnÃ©es (Conversation, Message)
â”œâ”€â”€ frontend/                  # Interface React
â”‚   â””â”€â”€ src/components/       # Composants UI
â”œâ”€â”€ venv/                     # Python 3.13 pour Django
â”œâ”€â”€ venv_vllm/               # Python 3.12 pour vLLM
â””â”€â”€ requirements.txt         # DÃ©pendances Django
```

## ğŸ” Variables d'environnement

```env
# Backend
OPENROUTER_API_KEY=sk-or-v1-xxxxx
SERPAPI_API_KEY=xxxxx
VLLM_BASE_URL=http://localhost:8080
VLLM_MODEL=microsoft/Phi-3-mini-4k-instruct

# Redis (production)
REDIS_HOST=chatbot-redis.redis.cache.windows.net
REDIS_KEY=xxxxx

# Database (production)
DATABASE_URL=postgresql://user:pass@server/db
```

## âš ï¸ Limites et quotas

- **OpenRouter (gratuit)** : ~10 requÃªtes/minute, 200/jour
- **SerpAPI (gratuit)** : 100 recherches/mois
- **Solution** : Utiliser vLLM local quand limite atteinte

## ğŸ“Š Performances

- **OpenRouter Cloud** : 2-5s par rÃ©ponse
- **vLLM Local (CPU)** : 30s-2min par rÃ©ponse âš ï¸
- **vLLM Local (GPU)** : 2-10s par rÃ©ponse
- **Recherche web** : <2s avec cache

## ğŸ¤ Points forts de l'implÃ©mentation

1. **Architecture modulaire** : Services dÃ©couplÃ©s et rÃ©utilisables
2. **Double mode LLM** : FlexibilitÃ© local/cloud
3. **UX moderne** : Interface intuitive avec feedback temps rÃ©el
4. **Production-ready** : Gestion d'erreurs, logs, monitoring
5. **SÃ©curitÃ©** : Secrets externalisÃ©s, CORS configurÃ©

---

# Partie 3 : Questions thÃ©oriques complÃ©mentaires

## Question 3.2 : AmÃ©lioration de la vitesse de traitement

### StratÃ©gies d'optimisation pour rÃ©duire la latence

#### 1. Optimisation du modÃ¨le
- **Quantization** : RÃ©duction INT8/INT4 (â†“75% mÃ©moire, â†‘2-4x vitesse)
- **Model pruning** : Suppression des poids non essentiels
- **Knowledge distillation** : ModÃ¨le plus petit entraÃ®nÃ© sur le grand

#### 2. Architecture optimisÃ©e
```
Client â†’ Load Balancer â†’ API Gateway
              â†“                â†“
      Fast Router (Phi-2)  vLLM Cluster (Phi-3)
              â†“                â†“
          Cache Layer (Redis + Vector DB)
```

#### 3. Techniques d'accÃ©lÃ©ration
- **Streaming des rÃ©ponses** : L'utilisateur voit la rÃ©ponse se construire
- **Batching dynamique** : Traitement groupÃ© des requÃªtes
- **Cache sÃ©mantique** : RÃ©utilisation des rÃ©ponses similaires
- **PrÃ©fixe caching vLLM** : `--enable-prefix-caching`

#### 4. MÃ©triques attendues
| Optimisation | Gain de performance |
|--------------|-------------------|
| Quantization 4-bit | 4x plus rapide |
| Streaming | Perception 50% plus rapide |
| Cache intelligent | 80% hit rate |
| **Total** | **5-15s â†’ 1-3s** |

---

## Question 4 : Architecture VLM (Vision-Language Model)

### IntÃ©gration d'un modÃ¨le de vision dans le chatbot

#### 1. Architecture proposÃ©e
```python
# services/vlm_service.py
class VLMService:
    def __init__(self):
        self.model = "llava-hf/llava-1.5-7b-hf"
        self.processor = AutoProcessor.from_pretrained(self.model)
        
    async def analyze_image(self, image_path: str, prompt: str):
        image = Image.open(image_path)
        inputs = self.processor(text=prompt, images=image, return_tensors="pt")
        
        output = self.model.generate(**inputs, max_new_tokens=500)
        return self.processor.decode(output[0], skip_special_tokens=True)
```

#### 2. Endpoint Django
```python
class ImageAnalysisView(APIView):
    parser_classes = (MultiPartParser,)
    
    async def post(self, request):
        image_file = request.FILES.get('image')
        prompt = request.data.get('prompt', 'Describe this image')
        
        # Analyse avec VLM
        vlm_service = VLMService()
        result = await vlm_service.analyze_image(temp_path, prompt)
        
        return Response({'analysis': result})
```

#### 3. Frontend avec upload
```typescript
const ImageUploader: React.FC = () => {
    const [preview, setPreview] = useState<string | null>(null);
    
    const handleUpload = async (file: File) => {
        const formData = new FormData();
        formData.append('image', file);
        
        const response = await axios.post('/api/v1/analyze-image/', formData);
        setAnalysis(response.data.analysis);
    };
}
```

#### 4. ModÃ¨les VLM recommandÃ©s
| ModÃ¨le | Taille | Use Case |
|--------|--------|----------|
| CLIP | 400M | Classification rapide |
| BLIP-2 | 3B | GÃ©nÃ©ration de lÃ©gendes |
| LLaVA-1.5 | 7B | Analyse dÃ©taillÃ©e |
| CogVLM | 17B | Questions complexes |

#### 5. Optimisations spÃ©cifiques
- **PrÃ©traitement** : Redimensionnement intelligent des images
- **Cache** : Hash d'image pour Ã©viter retraitement
- **Batching** : Traitement groupÃ© des images
- **Quantization** : ModÃ¨les 4-bit pour performance

### DÃ©fis et solutions
1. **Latence** â†’ ModÃ¨le quantizÃ© + streaming
2. **MÃ©moire GPU** â†’ Offloading CPU dynamique
3. **ScalabilitÃ©** â†’ Queue Celery + workers GPU